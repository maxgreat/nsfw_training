{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "MEAN_PREPRO = [0.485, 0.456, 0.406]\n",
    "STD_PREPRO = [0.229, 0.224, 0.225]\n",
    "RESIZE_PREPRO = 256,256\n",
    "RESIZE_DRAW = 256,256\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TRAIN_SHUFFLE = True\n",
    "TRAIN_NUM_WORKERS = 8\n",
    "TRAIN_PIN_MEMORY = True\n",
    "\n",
    "VAL_BATCH_SIZE = 512\n",
    "VAL_SHUFFLE = False\n",
    "VAL_NUM_WORKERS = 4\n",
    "VAL_PIN_MEMORY = True\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "DEVICE_ID = 1\n",
    "\n",
    "DATA_DIR = '/data/porn/binary/'\n",
    "TRAINSET_ROOT = f'{DATA_DIR}/train'\n",
    "TESTSET_ROOT = f'{DATA_DIR}/test'\n",
    "\n",
    "TENSORBOARD_DIR = '/data/porn/tensorboard/resnext101/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "        mean=MEAN_PREPRO, std=STD_PREPRO\n",
    "    )\n",
    "prepro = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "prepro_val = transforms.Compose(\n",
    "    [transforms.Resize(RESIZE_PREPRO), transforms.ToTensor(), normalize]\n",
    ")\n",
    "\n",
    "prepro_draw = transforms.Compose(\n",
    "    [transforms.Resize(RESIZE_DRAW), transforms.ToTensor()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class validation_dataset(ImageFolder):\n",
    "    def __init__(self, root, transform):\n",
    "        super().__init__(root, transform)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data, label = super().__getitem__(index)\n",
    "        return data, label, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(TRAINSET_ROOT, transform=prepro)\n",
    "val_dataset = validation_dataset(TESTSET_ROOT, transform=prepro_val)\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=TRAIN_SHUFFLE, num_workers=TRAIN_NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=VAL_SHUFFLE, num_workers=VAL_NUM_WORKERS, pin_memory=VAL_PIN_MEMORY, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/checkpoints/resnext101_32x8d-8ba56ff5.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d1547689d84b3da7b8ed29d8eef461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=356082095), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnext101_32x8d(pretrained=True)\n",
    "model.fc = torch.nn.Linear(2048, len(train_dataset.classes))\n",
    "DEVICE = f\"cuda:{DEVICE_ID}\"\n",
    "model = model.to(DEVICE)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=INITIAL_LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(classes, cm):\n",
    "    fig, ax = plt.subplots(figsize=(len(classes),len(classes)))  \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt=\"d\")\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels') \n",
    "    ax.set_title('Confusion Matrix') \n",
    "    ax.xaxis.set_ticklabels(classes,rotation=90)\n",
    "    ax.yaxis.set_ticklabels(classes,rotation=0)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    on_iteration=None,\n",
    "):\n",
    "    model = model.train()\n",
    "    end = time.time()\n",
    "    print(\"Start Training\")\n",
    "    avg_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        print(f\"{i/len(train_loader) * 100 : 2.2f}%\", end=\"\\r\")\n",
    "        iteration_time = time.time()\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if on_iteration is not None:\n",
    "            on_iteration(iteration=i+epoch*len(train_loader), loss=loss, y_pred=outputs, y_true=labels)     \n",
    "    return avg_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, \n",
    "             model, \n",
    "             criterion,\n",
    "             print_freq=1000):\n",
    "    model = model.eval()\n",
    "    y_pred, y_true, proba_pred, indexes = [], [], [], []\n",
    "    avg_loss = 0\n",
    "    for i, (inputs, labels, image_indexes) in enumerate(val_loader):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            avg_loss += criterion(outputs, labels).item()\n",
    "        proba_pred.append(outputs.cpu().clone())\n",
    "        _, indices_max = torch.max(outputs.cpu(), 1)\n",
    "        y_pred.append(indices_max)\n",
    "        y_true.append(labels.cpu().clone())\n",
    "        indexes.append(image_indexes.clone())\n",
    "    return {\"loss\":avg_loss/len(val_loader), \n",
    "            \"class_prediction\":torch.cat(y_pred), \n",
    "            \"ground_truth\":torch.cat(y_true), \n",
    "            \"probabilities\":torch.cat(proba_pred), \n",
    "            \"images_index\":torch.LongTensor(torch.cat(indexes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_max_error(epoch, val_res, n=5):\n",
    "    # max false positive\n",
    "    fp = (val_res[\"class_prediction\"] == 0) & (val_res[\"ground_truth\"] == 1)\n",
    "    values, images = [], []\n",
    "    for i in range(len(fp)):\n",
    "        if fp[i]:\n",
    "            values.append(val_res[\"probabilities\"][i][1])\n",
    "            images.append(val_dataset.imgs[val_res[\"images_index\"][i]][0])\n",
    "    _, max_indexes = torch.sort(torch.Tensor(values), descending=True)\n",
    "    for i in range(n):\n",
    "        logger.add_image(f\"Eval/FalsePositive/{i}\", prepro_draw(Image.open(images[max_indexes[i]])), epoch)\n",
    "        \n",
    "    #false negative\n",
    "    fn = (val_res[\"class_prediction\"] == 1) & (val_res[\"ground_truth\"] == 0)\n",
    "    values, images = [], []\n",
    "    for i in range(len(fn)):\n",
    "        if fn[i]:\n",
    "            values.append(val_res[\"probabilities\"][i][0])\n",
    "            images.append(val_dataset.imgs[val_res[\"images_index\"][i]][0])\n",
    "    _, max_indexes = torch.sort(torch.Tensor(values), descending=True)\n",
    "    for i in range(n):\n",
    "        logger.add_image(f\"Eval/FalseNegative/{i}\", prepro_draw(Image.open(images[max_indexes[i]])), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(val_res):\n",
    "    VP = (val_res[\"class_prediction\"] == 0) & (val_res[\"ground_truth\"] == 0)\n",
    "    FP = (val_res[\"class_prediction\"] == 0) & (val_res[\"ground_truth\"] == 1)\n",
    "    FN = (val_res[\"class_prediction\"] == 1) & (val_res[\"ground_truth\"] == 0)\n",
    "    vp = float(VP.sum())\n",
    "    fp = FP.sum()\n",
    "    fn = FN.sum()\n",
    "    precision = vp / (vp + fp)\n",
    "    recall = vp / (vp + fn)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_logs(epoch, validation_res):\n",
    "    logger.add_scalar(\"Loss/Avg_Val\", validation_res[\"loss\"], epoch)\n",
    "    logger.add_scalar(\"Eval/Accuracy\", (validation_res[\"class_prediction\"]==validation_res[\"ground_truth\"]).sum()/float(len(validation_res[\"ground_truth\"])), epoch)\n",
    "    precision, recall = compute_score(validation_res)\n",
    "    logger.add_scalar(\"Eval/Recall\", recall, epoch)\n",
    "    logger.add_scalar(\"Eval/Precision\", precision, epoch)\n",
    "    probas = [x[validation_res[\"ground_truth\"][i]] for i,x in enumerate(validation_res[\"probabilities\"])]\n",
    "    logger.add_pr_curve(\"Eval/Prec_recall\", validation_res[\"ground_truth\"], probas, epoch)\n",
    "    draw_max_error(epoch, validation_res)\n",
    "    cm = confusion_matrix(validation_res[\"class_prediction\"], validation_res[\"ground_truth\"])\n",
    "    cm_image = plot_cm(train_dataset.classes, cm)\n",
    "    logger.add_figure(\"Eval/Confusion Matrix\", cm_image, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_iteration_logs(iteration, loss, y_pred, y_true):\n",
    "    l = loss.item()\n",
    "    if iteration%50 == 0:\n",
    "        logger.add_scalar(\"Loss/Train\", l, iteration)\n",
    "        print(\n",
    "                f\"{iteration}/{len(train_loader)} \\t\"\n",
    "                f\"Loss {l}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = SummaryWriter(TENSORBOARD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    val_res = validate(val_loader, model, criterion)\n",
    "    validation_logs(i, val_res)\n",
    "    loss = train(\n",
    "                    train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    i,\n",
    "                    on_iteration=on_iteration_logs,\n",
    "                )\n",
    "    logger.add_scalar(\"Loss/Avg_Train\", loss, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, child in enumerate(model.children()):\n",
    "    if i > 6:\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE//2, shuffle=TRAIN_SHUFFLE, num_workers=TRAIN_NUM_WORKERS, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=INITIAL_LR*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "28650/2862 \tLoss 0.1686488538980484\n",
      "28700/2862 \tLoss 0.2095438539981842\n",
      "28750/2862 \tLoss 0.23723465204238892\n",
      "28800/2862 \tLoss 0.24509967863559723\n",
      "28850/2862 \tLoss 0.12935705482959747\n",
      "28900/2862 \tLoss 0.22119750082492828\n",
      "28950/2862 \tLoss 0.2232341766357422\n",
      "29000/2862 \tLoss 0.1304795742034912\n",
      "29050/2862 \tLoss 0.18364578485488892\n",
      "29100/2862 \tLoss 0.2123449593782425\n",
      "29150/2862 \tLoss 0.21678024530410767\n",
      "29200/2862 \tLoss 0.15836134552955627\n",
      "29250/2862 \tLoss 0.16371679306030273\n",
      "29300/2862 \tLoss 0.15101487934589386\n",
      "29350/2862 \tLoss 0.23681676387786865\n",
      "29400/2862 \tLoss 0.22939768433570862\n",
      "29450/2862 \tLoss 0.22951573133468628\n",
      "29500/2862 \tLoss 0.20864397287368774\n",
      "29550/2862 \tLoss 0.11768437176942825\n",
      "29600/2862 \tLoss 0.17191599309444427\n",
      "29650/2862 \tLoss 0.12864845991134644\n",
      "29700/2862 \tLoss 0.17533104121685028\n",
      "29750/2862 \tLoss 0.17011667788028717\n",
      "29800/2862 \tLoss 0.074493907392025\n",
      "29850/2862 \tLoss 0.15106551349163055\n",
      "29900/2862 \tLoss 0.10724467039108276\n",
      "29950/2862 \tLoss 0.1855291724205017\n",
      "30000/2862 \tLoss 0.2625005543231964\n",
      "30050/2862 \tLoss 0.12484146654605865\n",
      "30100/2862 \tLoss 0.22005081176757812\n",
      "30150/2862 \tLoss 0.13904334604740143\n",
      "30200/2862 \tLoss 0.15968264639377594\n",
      "30250/2862 \tLoss 0.188078373670578\n",
      "30300/2862 \tLoss 0.16931471228599548\n",
      "30350/2862 \tLoss 0.1425810158252716\n",
      "30400/2862 \tLoss 0.21330296993255615\n",
      "30450/2862 \tLoss 0.11520955711603165\n",
      "30500/2862 \tLoss 0.15068049728870392\n",
      "30550/2862 \tLoss 0.14489862322807312\n",
      "30600/2862 \tLoss 0.13851672410964966\n",
      "30650/2862 \tLoss 0.2072421759366989\n",
      "30700/2862 \tLoss 0.14842917025089264\n",
      "30750/2862 \tLoss 0.09937149286270142\n",
      "30800/2862 \tLoss 0.11040494590997696\n",
      "30850/2862 \tLoss 0.14520056545734406\n",
      "30900/2862 \tLoss 0.1547587811946869\n",
      "30950/2862 \tLoss 0.09804181754589081\n",
      "31000/2862 \tLoss 0.1404302418231964\n",
      "31050/2862 \tLoss 0.0854177251458168\n",
      "31100/2862 \tLoss 0.14689859747886658\n",
      "31150/2862 \tLoss 0.06788685917854309\n",
      "31200/2862 \tLoss 0.20862329006195068\n",
      "31250/2862 \tLoss 0.2120189219713211\n",
      "31300/2862 \tLoss 0.10683231055736542\n",
      "31350/2862 \tLoss 0.08546170592308044\n",
      "31400/2862 \tLoss 0.1494867503643036\n",
      "31450/2862 \tLoss 0.08013126999139786\n",
      "Start Training\n",
      "31500/2862 \tLoss 0.2005036622285843\n",
      "31550/2862 \tLoss 0.13991403579711914\n",
      "31600/2862 \tLoss 0.1890205442905426\n",
      "31650/2862 \tLoss 0.22004932165145874\n",
      "31700/2862 \tLoss 0.15431444346904755\n",
      "31750/2862 \tLoss 0.14702607691287994\n",
      "31800/2862 \tLoss 0.20970012247562408\n",
      "31850/2862 \tLoss 0.17938169836997986\n",
      "31900/2862 \tLoss 0.16322901844978333\n",
      "31950/2862 \tLoss 0.09423619508743286\n",
      "32000/2862 \tLoss 0.1394951492547989\n",
      "32050/2862 \tLoss 0.15835177898406982\n",
      "32100/2862 \tLoss 0.08332612365484238\n",
      "32150/2862 \tLoss 0.05744938179850578\n",
      "32200/2862 \tLoss 0.16285187005996704\n",
      "32250/2862 \tLoss 0.06014593690633774\n",
      "32300/2862 \tLoss 0.1797502636909485\n",
      "32350/2862 \tLoss 0.11883200705051422\n",
      "32400/2862 \tLoss 0.07844273746013641\n",
      "32450/2862 \tLoss 0.1615922898054123\n",
      "32500/2862 \tLoss 0.07620128989219666\n",
      "32550/2862 \tLoss 0.10278291255235672\n",
      "32600/2862 \tLoss 0.05846168100833893\n",
      "32650/2862 \tLoss 0.10673926770687103\n",
      "32700/2862 \tLoss 0.15382304787635803\n",
      "32750/2862 \tLoss 0.06038321927189827\n",
      "32800/2862 \tLoss 0.10143301635980606\n",
      "32850/2862 \tLoss 0.04949966445565224\n",
      "32900/2862 \tLoss 0.089630126953125\n",
      "32950/2862 \tLoss 0.1872294694185257\n",
      "33000/2862 \tLoss 0.11307910084724426\n",
      "33050/2862 \tLoss 0.07675269991159439\n",
      "33100/2862 \tLoss 0.18675406277179718\n",
      "33150/2862 \tLoss 0.1280735731124878\n",
      "33200/2862 \tLoss 0.07044975459575653\n",
      "33250/2862 \tLoss 0.21344220638275146\n",
      "33300/2862 \tLoss 0.061964333057403564\n",
      "33350/2862 \tLoss 0.09030980616807938\n",
      "33400/2862 \tLoss 0.19460202753543854\n",
      "33450/2862 \tLoss 0.06866826117038727\n",
      "33500/2862 \tLoss 0.10869965702295303\n",
      "33550/2862 \tLoss 0.24685724079608917\n",
      "33600/2862 \tLoss 0.14568090438842773\n",
      "33650/2862 \tLoss 0.1303112506866455\n",
      "33700/2862 \tLoss 0.10836324095726013\n",
      "33750/2862 \tLoss 0.06417796015739441\n",
      "33800/2862 \tLoss 0.05636442452669144\n",
      "33850/2862 \tLoss 0.11769992858171463\n",
      "33900/2862 \tLoss 0.1050545871257782\n",
      "33950/2862 \tLoss 0.13265962898731232\n",
      "34000/2862 \tLoss 0.1674552857875824\n",
      "34050/2862 \tLoss 0.10053312033414841\n",
      "34100/2862 \tLoss 0.12668848037719727\n",
      "34150/2862 \tLoss 0.06863108277320862\n",
      "34200/2862 \tLoss 0.10326629877090454\n",
      "34250/2862 \tLoss 0.20413634181022644\n",
      "34300/2862 \tLoss 0.09215100109577179\n",
      "Start Training\n",
      "34350/2862 \tLoss 0.11149520426988602\n",
      "34400/2862 \tLoss 0.08309751003980637\n",
      "34450/2862 \tLoss 0.2398776113986969\n",
      "34500/2862 \tLoss 0.07104448229074478\n",
      "34550/2862 \tLoss 0.0633075088262558\n",
      "34600/2862 \tLoss 0.22462743520736694\n",
      "34650/2862 \tLoss 0.06195642799139023\n",
      "34700/2862 \tLoss 0.06869765371084213\n",
      "34750/2862 \tLoss 0.1552668809890747\n",
      "34800/2862 \tLoss 0.2793236970901489\n",
      "34850/2862 \tLoss 0.14559821784496307\n",
      "34900/2862 \tLoss 0.14045897126197815\n",
      "34950/2862 \tLoss 0.07807893306016922\n",
      "35000/2862 \tLoss 0.10610638558864594\n",
      "35050/2862 \tLoss 0.10824209451675415\n",
      "35100/2862 \tLoss 0.09030504524707794\n",
      "35150/2862 \tLoss 0.11362548917531967\n",
      "35200/2862 \tLoss 0.13419011235237122\n",
      "35250/2862 \tLoss 0.0967453196644783\n",
      "35300/2862 \tLoss 0.12279285490512848\n",
      "35350/2862 \tLoss 0.3950560390949249\n",
      "35400/2862 \tLoss 0.1254960149526596\n",
      "35450/2862 \tLoss 0.17178292572498322\n",
      "35500/2862 \tLoss 0.08813189715147018\n",
      "35550/2862 \tLoss 0.07261490821838379\n",
      "35600/2862 \tLoss 0.1452413946390152\n",
      "35650/2862 \tLoss 0.13340641558170319\n",
      "35700/2862 \tLoss 0.11931072175502777\n",
      "35750/2862 \tLoss 0.145482137799263\n",
      "35800/2862 \tLoss 0.15192845463752747\n",
      "35850/2862 \tLoss 0.12333625555038452\n",
      "35900/2862 \tLoss 0.11999095976352692\n",
      "35950/2862 \tLoss 0.05372537672519684\n",
      "36000/2862 \tLoss 0.12333685159683228\n",
      "36050/2862 \tLoss 0.17305989563465118\n",
      "36100/2862 \tLoss 0.1568920612335205\n",
      "36150/2862 \tLoss 0.08027703315019608\n",
      "36200/2862 \tLoss 0.1792837232351303\n",
      "36250/2862 \tLoss 0.08311668038368225\n",
      "36300/2862 \tLoss 0.10211421549320221\n",
      "36350/2862 \tLoss 0.15061207115650177\n",
      "36400/2862 \tLoss 0.14170873165130615\n",
      "36450/2862 \tLoss 0.13726073503494263\n",
      "36500/2862 \tLoss 0.09310830384492874\n",
      "36550/2862 \tLoss 0.13745880126953125\n",
      "36600/2862 \tLoss 0.03520951792597771\n",
      "36650/2862 \tLoss 0.11887788027524948\n",
      "36700/2862 \tLoss 0.07807085663080215\n",
      "36750/2862 \tLoss 0.2164112627506256\n",
      "36800/2862 \tLoss 0.15412533283233643\n",
      "36850/2862 \tLoss 0.37275680899620056\n",
      "36900/2862 \tLoss 0.11763186752796173\n",
      "36950/2862 \tLoss 0.04572476074099541\n",
      "37000/2862 \tLoss 0.09592422842979431\n",
      "37050/2862 \tLoss 0.08318935334682465\n",
      "37100/2862 \tLoss 0.15630456805229187\n",
      "37150/2862 \tLoss 0.10027848929166794\n",
      "37200/2862 \tLoss 0.1404167264699936\n",
      "Start Training\n",
      "37250/2862 \tLoss 0.17876525223255157\n",
      "37300/2862 \tLoss 0.07337494194507599\n",
      "37350/2862 \tLoss 0.11692129075527191\n",
      "37400/2862 \tLoss 0.09845670312643051\n",
      "37450/2862 \tLoss 0.12718957662582397\n",
      "37500/2862 \tLoss 0.17279163002967834\n",
      "37550/2862 \tLoss 0.08419861644506454\n",
      "37600/2862 \tLoss 0.05827198550105095\n",
      "37650/2862 \tLoss 0.09028718620538712\n",
      "37700/2862 \tLoss 0.13898272812366486\n",
      "37750/2862 \tLoss 0.14039775729179382\n",
      "37800/2862 \tLoss 0.09173019975423813\n",
      "37850/2862 \tLoss 0.1212938129901886\n",
      "37900/2862 \tLoss 0.06095682084560394\n",
      "37950/2862 \tLoss 0.28605878353118896\n",
      "38000/2862 \tLoss 0.09909991174936295\n",
      "38050/2862 \tLoss 0.08513487875461578\n",
      "38100/2862 \tLoss 0.09081835299730301\n",
      "38150/2862 \tLoss 0.05046750232577324\n",
      "38200/2862 \tLoss 0.10309558361768723\n",
      "38250/2862 \tLoss 0.1256873607635498\n",
      "38300/2862 \tLoss 0.1429791897535324\n",
      "38350/2862 \tLoss 0.056014541536569595\n",
      "38400/2862 \tLoss 0.11512394994497299\n",
      "38450/2862 \tLoss 0.0493781752884388\n",
      "38500/2862 \tLoss 0.18345427513122559\n",
      "38550/2862 \tLoss 0.27416977286338806\n",
      "38600/2862 \tLoss 0.13784925639629364\n",
      "38650/2862 \tLoss 0.2237456738948822\n",
      "38700/2862 \tLoss 0.18655502796173096\n",
      "38750/2862 \tLoss 0.06669721752405167\n",
      "38800/2862 \tLoss 0.11990015208721161\n",
      "38850/2862 \tLoss 0.13655373454093933\n",
      "38900/2862 \tLoss 0.0926123708486557\n",
      "38950/2862 \tLoss 0.12026981264352798\n",
      "39000/2862 \tLoss 0.14900431036949158\n",
      "39050/2862 \tLoss 0.10615509748458862\n",
      "39100/2862 \tLoss 0.04089140146970749\n",
      "39150/2862 \tLoss 0.05158206820487976\n",
      "39200/2862 \tLoss 0.15082351863384247\n",
      "39250/2862 \tLoss 0.09419715404510498\n",
      "39300/2862 \tLoss 0.07140050083398819\n",
      "39350/2862 \tLoss 0.11671429872512817\n",
      "39400/2862 \tLoss 0.046692799776792526\n",
      "39450/2862 \tLoss 0.26396650075912476\n",
      "39500/2862 \tLoss 0.06397083401679993\n",
      "39550/2862 \tLoss 0.11459002643823624\n",
      "39600/2862 \tLoss 0.17596270143985748\n",
      "39650/2862 \tLoss 0.1421041488647461\n",
      "39700/2862 \tLoss 0.14537939429283142\n",
      "39750/2862 \tLoss 0.1931244432926178\n",
      "39800/2862 \tLoss 0.0871727392077446\n",
      "39850/2862 \tLoss 0.09526148438453674\n",
      "39900/2862 \tLoss 0.13610318303108215\n",
      "39950/2862 \tLoss 0.045690055936574936\n",
      "40000/2862 \tLoss 0.09379124641418457\n",
      "40050/2862 \tLoss 0.07283871620893478\n",
      "Start Training\n",
      "40100/2862 \tLoss 0.046132609248161316\n",
      "40150/2862 \tLoss 0.06264903396368027\n",
      "40200/2862 \tLoss 0.07908870279788971\n",
      "40250/2862 \tLoss 0.05938752740621567\n",
      "40300/2862 \tLoss 0.15187504887580872\n",
      "40350/2862 \tLoss 0.05858641117811203\n",
      "40400/2862 \tLoss 0.13149921596050262\n",
      "40450/2862 \tLoss 0.04718441888689995\n",
      "40500/2862 \tLoss 0.14887265861034393\n",
      "40550/2862 \tLoss 0.07246418297290802\n",
      "40600/2862 \tLoss 0.06225835159420967\n",
      "40650/2862 \tLoss 0.06218183413147926\n",
      "40700/2862 \tLoss 0.09310305118560791\n",
      "40750/2862 \tLoss 0.08437047898769379\n",
      "40800/2862 \tLoss 0.08429774641990662\n",
      "40850/2862 \tLoss 0.10061419010162354\n",
      "40900/2862 \tLoss 0.13427123427391052\n",
      "40950/2862 \tLoss 0.21669182181358337\n",
      "41000/2862 \tLoss 0.12225338816642761\n",
      "41050/2862 \tLoss 0.03652563691139221\n",
      "41100/2862 \tLoss 0.11457537859678268\n",
      "41150/2862 \tLoss 0.1624813973903656\n",
      "41200/2862 \tLoss 0.10907753556966782\n",
      "41250/2862 \tLoss 0.03793798014521599\n",
      "41300/2862 \tLoss 0.12848907709121704\n",
      "41350/2862 \tLoss 0.09168946743011475\n",
      "41400/2862 \tLoss 0.076494500041008\n",
      "41450/2862 \tLoss 0.13629569113254547\n",
      "41500/2862 \tLoss 0.14692962169647217\n",
      "41550/2862 \tLoss 0.15177956223487854\n",
      "41600/2862 \tLoss 0.06818874180316925\n",
      "41650/2862 \tLoss 0.10436590015888214\n",
      "41700/2862 \tLoss 0.08906641602516174\n",
      "41750/2862 \tLoss 0.041435420513153076\n",
      "41800/2862 \tLoss 0.03501752018928528\n",
      "41850/2862 \tLoss 0.16388331353664398\n",
      "41900/2862 \tLoss 0.10369215905666351\n",
      "41950/2862 \tLoss 0.12377654016017914\n",
      "42000/2862 \tLoss 0.07962006330490112\n",
      "42050/2862 \tLoss 0.07424851506948471\n",
      "42100/2862 \tLoss 0.11188174784183502\n",
      "42150/2862 \tLoss 0.13908836245536804\n",
      "42200/2862 \tLoss 0.08451800048351288\n",
      "42250/2862 \tLoss 0.05741003528237343\n",
      "42300/2862 \tLoss 0.09884762763977051\n",
      "42350/2862 \tLoss 0.028359640389680862\n",
      "42400/2862 \tLoss 0.14178000390529633\n",
      "42450/2862 \tLoss 0.09358087927103043\n",
      "42500/2862 \tLoss 0.09799458831548691\n",
      "42550/2862 \tLoss 0.19097626209259033\n",
      "42600/2862 \tLoss 0.09238927811384201\n",
      "42650/2862 \tLoss 0.08031275868415833\n",
      "42700/2862 \tLoss 0.09875895082950592\n",
      "42750/2862 \tLoss 0.08382586389780045\n",
      "42800/2862 \tLoss 0.11490745097398758\n",
      "42850/2862 \tLoss 0.0773012712597847\n",
      "42900/2862 \tLoss 0.03055547922849655\n",
      "Start Training\n",
      "42950/2862 \tLoss 0.17475613951683044\n",
      "43000/2862 \tLoss 0.0625750795006752\n",
      "43050/2862 \tLoss 0.12490175664424896\n",
      "43100/2862 \tLoss 0.15902386605739594\n",
      "43150/2862 \tLoss 0.08920574933290482\n",
      "43200/2862 \tLoss 0.07499893754720688\n",
      "43250/2862 \tLoss 0.0858229398727417\n",
      "43300/2862 \tLoss 0.1742883026599884\n",
      "43350/2862 \tLoss 0.09488241374492645\n",
      "43400/2862 \tLoss 0.24743255972862244\n",
      "43450/2862 \tLoss 0.08046631515026093\n",
      "43500/2862 \tLoss 0.14678965508937836\n",
      "43550/2862 \tLoss 0.11104953289031982\n",
      "43600/2862 \tLoss 0.07744793593883514\n",
      "43650/2862 \tLoss 0.023023460060358047\n",
      "43700/2862 \tLoss 0.11481118947267532\n",
      "43750/2862 \tLoss 0.07600527256727219\n",
      "43800/2862 \tLoss 0.0983433723449707\n",
      "43850/2862 \tLoss 0.07224977016448975\n",
      "43900/2862 \tLoss 0.10992592573165894\n",
      "43950/2862 \tLoss 0.10133253037929535\n",
      "44000/2862 \tLoss 0.20611457526683807\n",
      "44050/2862 \tLoss 0.13655194640159607\n",
      "44100/2862 \tLoss 0.08285939693450928\n",
      "44150/2862 \tLoss 0.11361338198184967\n",
      "44200/2862 \tLoss 0.07232224196195602\n",
      "44250/2862 \tLoss 0.06638344377279282\n",
      "44300/2862 \tLoss 0.07879826426506042\n",
      "44350/2862 \tLoss 0.056550245732069016\n",
      "44400/2862 \tLoss 0.06789868324995041\n",
      "44450/2862 \tLoss 0.07421811670064926\n",
      "44500/2862 \tLoss 0.0795913115143776\n",
      "44550/2862 \tLoss 0.08011461794376373\n",
      "44600/2862 \tLoss 0.1206396222114563\n",
      "44650/2862 \tLoss 0.09825293719768524\n",
      "44700/2862 \tLoss 0.09099870175123215\n",
      "44750/2862 \tLoss 0.1240507960319519\n",
      "44800/2862 \tLoss 0.15476329624652863\n",
      "44850/2862 \tLoss 0.19504785537719727\n",
      "44900/2862 \tLoss 0.13330614566802979\n",
      "44950/2862 \tLoss 0.18839827179908752\n",
      "45000/2862 \tLoss 0.12249928712844849\n",
      "45050/2862 \tLoss 0.04370812326669693\n",
      "45100/2862 \tLoss 0.11772917211055756\n",
      "45150/2862 \tLoss 0.1107681468129158\n",
      "45200/2862 \tLoss 0.04811212792992592\n",
      "45250/2862 \tLoss 0.08333823084831238\n",
      "45300/2862 \tLoss 0.05704794079065323\n",
      "45350/2862 \tLoss 0.11418189853429794\n",
      "45400/2862 \tLoss 0.08081025630235672\n",
      "45450/2862 \tLoss 0.12956850230693817\n",
      "45500/2862 \tLoss 0.03431357070803642\n",
      "45550/2862 \tLoss 0.09592494368553162\n",
      "45600/2862 \tLoss 0.17450153827667236\n",
      "45650/2862 \tLoss 0.04319610819220543\n",
      "45700/2862 \tLoss 0.06494879722595215\n",
      "45750/2862 \tLoss 0.07419159263372421\n",
      "Start Training\n",
      "45800/2862 \tLoss 0.19364458322525024\n",
      "45850/2862 \tLoss 0.11922033131122589\n",
      "45900/2862 \tLoss 0.11262428760528564\n",
      "45950/2862 \tLoss 0.030571546405553818\n",
      "46000/2862 \tLoss 0.15331421792507172\n",
      "46050/2862 \tLoss 0.05305181443691254\n",
      "46100/2862 \tLoss 0.07901895046234131\n",
      "46150/2862 \tLoss 0.14930035173892975\n",
      "46200/2862 \tLoss 0.10473167896270752\n",
      "46250/2862 \tLoss 0.06396058946847916\n",
      "46300/2862 \tLoss 0.13225241005420685\n",
      "46350/2862 \tLoss 0.06798159331083298\n",
      "46400/2862 \tLoss 0.09761050343513489\n",
      "46450/2862 \tLoss 0.17791542410850525\n",
      "46500/2862 \tLoss 0.1269611418247223\n",
      "46550/2862 \tLoss 0.08277572691440582\n",
      "46600/2862 \tLoss 0.15318360924720764\n",
      "46650/2862 \tLoss 0.08775153756141663\n",
      "46700/2862 \tLoss 0.09163190424442291\n",
      "46750/2862 \tLoss 0.02885787934064865\n",
      "46800/2862 \tLoss 0.1015051007270813\n",
      "46850/2862 \tLoss 0.14862629771232605\n",
      "46900/2862 \tLoss 0.102337546646595\n",
      "46950/2862 \tLoss 0.02702617458999157\n",
      "47000/2862 \tLoss 0.04815663397312164\n",
      "47050/2862 \tLoss 0.031444668769836426\n",
      "47100/2862 \tLoss 0.10581610351800919\n",
      "47150/2862 \tLoss 0.21982888877391815\n",
      "47200/2862 \tLoss 0.0973605066537857\n",
      "47250/2862 \tLoss 0.09731853753328323\n",
      "47300/2862 \tLoss 0.09573038667440414\n",
      "47350/2862 \tLoss 0.09325344860553741\n",
      "47400/2862 \tLoss 0.20513275265693665\n",
      "47450/2862 \tLoss 0.0159059576690197\n",
      "47500/2862 \tLoss 0.025932079181075096\n",
      "47550/2862 \tLoss 0.03082401119172573\n",
      "47600/2862 \tLoss 0.06451348215341568\n",
      "47650/2862 \tLoss 0.08269567042589188\n",
      "47700/2862 \tLoss 0.11398757994174957\n",
      "47750/2862 \tLoss 0.11018425226211548\n",
      "47800/2862 \tLoss 0.09064055979251862\n",
      "47850/2862 \tLoss 0.07658204436302185\n",
      "47900/2862 \tLoss 0.12269431352615356\n",
      "47950/2862 \tLoss 0.12961459159851074\n",
      "48000/2862 \tLoss 0.08018773794174194\n",
      "48050/2862 \tLoss 0.05218477547168732\n",
      "48100/2862 \tLoss 0.17172592878341675\n",
      "48150/2862 \tLoss 0.060909368097782135\n",
      "48200/2862 \tLoss 0.08754459768533707\n",
      "48250/2862 \tLoss 0.045718565583229065\n",
      "48300/2862 \tLoss 0.07012347131967545\n",
      "48350/2862 \tLoss 0.06232656538486481\n",
      "48400/2862 \tLoss 0.12011796236038208\n",
      "48450/2862 \tLoss 0.20876790583133698\n",
      "48500/2862 \tLoss 0.056580740958452225\n",
      "48550/2862 \tLoss 0.05182250216603279\n",
      "48600/2862 \tLoss 0.08157113194465637\n",
      "48650/2862 \tLoss 0.1153738871216774\n",
      "Start Training\n",
      "48700/2862 \tLoss 0.19435372948646545\n",
      "48750/2862 \tLoss 0.04112629219889641\n",
      "48800/2862 \tLoss 0.12098577618598938\n",
      "48850/2862 \tLoss 0.09338489174842834\n",
      "48900/2862 \tLoss 0.1699255257844925\n",
      "48950/2862 \tLoss 0.05975363776087761\n",
      "49000/2862 \tLoss 0.19832678139209747\n",
      "49050/2862 \tLoss 0.06347690522670746\n",
      "49100/2862 \tLoss 0.0676082894206047\n",
      "49150/2862 \tLoss 0.09984975308179855\n",
      "49200/2862 \tLoss 0.041988253593444824\n",
      "49250/2862 \tLoss 0.0746794044971466\n",
      "49300/2862 \tLoss 0.08962512761354446\n",
      "49350/2862 \tLoss 0.06491583585739136\n",
      "49400/2862 \tLoss 0.07122693955898285\n",
      "49450/2862 \tLoss 0.17042048275470734\n",
      "49500/2862 \tLoss 0.07628835737705231\n",
      "49550/2862 \tLoss 0.046401455998420715\n",
      "49600/2862 \tLoss 0.06391055881977081\n",
      "49650/2862 \tLoss 0.03438103571534157\n",
      "49700/2862 \tLoss 0.07078345119953156\n",
      "49750/2862 \tLoss 0.07721567153930664\n",
      "49800/2862 \tLoss 0.11829857528209686\n",
      "49850/2862 \tLoss 0.11552983522415161\n",
      "49900/2862 \tLoss 0.08053039014339447\n",
      "49950/2862 \tLoss 0.020030448213219643\n",
      "50000/2862 \tLoss 0.04111351817846298\n",
      "50050/2862 \tLoss 0.06700918823480606\n",
      "50100/2862 \tLoss 0.06904703378677368\n",
      "50150/2862 \tLoss 0.13200654089450836\n",
      "50200/2862 \tLoss 0.05187181755900383\n",
      "50250/2862 \tLoss 0.0601082518696785\n",
      "50300/2862 \tLoss 0.22239837050437927\n",
      "50350/2862 \tLoss 0.06226445734500885\n",
      "50400/2862 \tLoss 0.022030215710401535\n",
      "50450/2862 \tLoss 0.06400848925113678\n",
      "50500/2862 \tLoss 0.11396005749702454\n",
      "50550/2862 \tLoss 0.054616816341876984\n",
      "50600/2862 \tLoss 0.020045651122927666\n",
      "50650/2862 \tLoss 0.1434430032968521\n",
      "50700/2862 \tLoss 0.056389495730400085\n",
      "50750/2862 \tLoss 0.17430773377418518\n",
      "50800/2862 \tLoss 0.08889561891555786\n",
      "50850/2862 \tLoss 0.1374530792236328\n",
      "50900/2862 \tLoss 0.17363610863685608\n",
      "50950/2862 \tLoss 0.043078914284706116\n",
      "51000/2862 \tLoss 0.042585570365190506\n",
      "51050/2862 \tLoss 0.05963524430990219\n",
      "51100/2862 \tLoss 0.057866670191287994\n",
      "51150/2862 \tLoss 0.06969428807497025\n",
      "51200/2862 \tLoss 0.09074543416500092\n",
      "51250/2862 \tLoss 0.1047477200627327\n",
      "51300/2862 \tLoss 0.11788592487573624\n",
      "51350/2862 \tLoss 0.09320297837257385\n",
      "51400/2862 \tLoss 0.06457625329494476\n",
      "51450/2862 \tLoss 0.04597033932805061\n",
      "51500/2862 \tLoss 0.07332748919725418\n",
      "Start Training\n",
      "51550/2862 \tLoss 0.09916146099567413\n",
      "51600/2862 \tLoss 0.056459322571754456\n",
      "51650/2862 \tLoss 0.1444588303565979\n",
      "51700/2862 \tLoss 0.03938116505742073\n",
      "51750/2862 \tLoss 0.11007184535264969\n",
      "51800/2862 \tLoss 0.030042752623558044\n",
      "51850/2862 \tLoss 0.09730472415685654\n",
      "51900/2862 \tLoss 0.06135169044137001\n",
      "51950/2862 \tLoss 0.08557832986116409\n",
      "52000/2862 \tLoss 0.02610989287495613\n",
      "52050/2862 \tLoss 0.16593623161315918\n",
      "52100/2862 \tLoss 0.03998308628797531\n",
      "52150/2862 \tLoss 0.06118278577923775\n",
      "52200/2862 \tLoss 0.10305288434028625\n",
      "52250/2862 \tLoss 0.09886639565229416\n",
      "52300/2862 \tLoss 0.11001811921596527\n",
      "52350/2862 \tLoss 0.14771439135074615\n",
      "52400/2862 \tLoss 0.1870373785495758\n",
      "52450/2862 \tLoss 0.18138311803340912\n",
      "52500/2862 \tLoss 0.03983357548713684\n",
      "52550/2862 \tLoss 0.03351786360144615\n",
      "52600/2862 \tLoss 0.13170143961906433\n",
      "52650/2862 \tLoss 0.11428771913051605\n",
      "52700/2862 \tLoss 0.09569111466407776\n",
      "52750/2862 \tLoss 0.14809644222259521\n",
      "52800/2862 \tLoss 0.07510604709386826\n",
      "52850/2862 \tLoss 0.15297764539718628\n",
      "52900/2862 \tLoss 0.0808129608631134\n",
      "52950/2862 \tLoss 0.12524963915348053\n",
      "53000/2862 \tLoss 0.040820471942424774\n",
      "53050/2862 \tLoss 0.11826595664024353\n",
      "53100/2862 \tLoss 0.06003318727016449\n",
      "53150/2862 \tLoss 0.08619670569896698\n",
      "53200/2862 \tLoss 0.08608665317296982\n",
      "53250/2862 \tLoss 0.048157162964344025\n",
      "53300/2862 \tLoss 0.09145577251911163\n",
      "53350/2862 \tLoss 0.04454590752720833\n",
      "53400/2862 \tLoss 0.12611767649650574\n",
      "53450/2862 \tLoss 0.11370964348316193\n",
      "53500/2862 \tLoss 0.0842270776629448\n",
      "53550/2862 \tLoss 0.06640388071537018\n",
      "53600/2862 \tLoss 0.033538687974214554\n",
      "53650/2862 \tLoss 0.0852014571428299\n",
      "53700/2862 \tLoss 0.021862665191292763\n",
      "53750/2862 \tLoss 0.12838657200336456\n",
      "53800/2862 \tLoss 0.08142592012882233\n",
      "53850/2862 \tLoss 0.04719253256917\n",
      "53900/2862 \tLoss 0.1064765453338623\n",
      "53950/2862 \tLoss 0.09110499918460846\n",
      "54000/2862 \tLoss 0.15524059534072876\n",
      "54050/2862 \tLoss 0.07389937341213226\n",
      "54100/2862 \tLoss 0.047902487218379974\n",
      "54150/2862 \tLoss 0.09600654244422913\n",
      "54200/2862 \tLoss 0.029850497841835022\n",
      "54250/2862 \tLoss 0.05395602062344551\n",
      "54300/2862 \tLoss 0.15117532014846802\n",
      "54350/2862 \tLoss 0.05186913162469864\n",
      "Start Training\n",
      "54400/2862 \tLoss 0.02624976634979248\n",
      "54450/2862 \tLoss 0.0804796889424324\n",
      "54500/2862 \tLoss 0.08225740492343903\n",
      "54550/2862 \tLoss 0.062194425612688065\n",
      "54600/2862 \tLoss 0.12857939302921295\n",
      "54650/2862 \tLoss 0.07535837590694427\n",
      "54700/2862 \tLoss 0.0387578010559082\n",
      "54750/2862 \tLoss 0.09299275279045105\n",
      "54800/2862 \tLoss 0.19704745709896088\n",
      "54850/2862 \tLoss 0.10839709639549255\n",
      "54900/2862 \tLoss 0.07779490202665329\n",
      "54950/2862 \tLoss 0.0814826488494873\n",
      "55000/2862 \tLoss 0.03090601973235607\n",
      "55050/2862 \tLoss 0.06336486339569092\n",
      "55100/2862 \tLoss 0.09090545028448105\n",
      "55150/2862 \tLoss 0.1881149709224701\n",
      "55200/2862 \tLoss 0.10863196849822998\n",
      "55250/2862 \tLoss 0.04406728595495224\n",
      "55300/2862 \tLoss 0.04544464498758316\n",
      "55350/2862 \tLoss 0.09292292594909668\n",
      "55400/2862 \tLoss 0.060558002442121506\n",
      "55450/2862 \tLoss 0.049914371222257614\n",
      "55500/2862 \tLoss 0.11453366279602051\n",
      "55550/2862 \tLoss 0.08277926594018936\n",
      "55600/2862 \tLoss 0.13389195501804352\n",
      "55650/2862 \tLoss 0.09667333960533142\n",
      "55700/2862 \tLoss 0.10433297604322433\n",
      "55750/2862 \tLoss 0.12729068100452423\n",
      "55800/2862 \tLoss 0.05831203609704971\n",
      "55850/2862 \tLoss 0.021911077201366425\n",
      "55900/2862 \tLoss 0.0987851470708847\n",
      "55950/2862 \tLoss 0.11765189468860626\n",
      "56000/2862 \tLoss 0.03000028058886528\n",
      "56050/2862 \tLoss 0.11849269270896912\n",
      "56100/2862 \tLoss 0.033711861819028854\n",
      "56150/2862 \tLoss 0.10490687936544418\n",
      "56200/2862 \tLoss 0.10971949994564056\n",
      "56250/2862 \tLoss 0.035260606557130814\n",
      "56300/2862 \tLoss 0.11906520277261734\n",
      "56350/2862 \tLoss 0.03952578455209732\n",
      "56400/2862 \tLoss 0.055198684334754944\n",
      "56450/2862 \tLoss 0.043821729719638824\n",
      "56500/2862 \tLoss 0.0423826240003109\n",
      "56550/2862 \tLoss 0.3084357976913452\n",
      "56600/2862 \tLoss 0.048555366694927216\n",
      "56650/2862 \tLoss 0.16954205930233002\n",
      "56700/2862 \tLoss 0.08115671575069427\n",
      "56750/2862 \tLoss 0.04818655550479889\n",
      "56800/2862 \tLoss 0.261266827583313\n",
      "56850/2862 \tLoss 0.03699706494808197\n",
      "56900/2862 \tLoss 0.10606677085161209\n",
      "56950/2862 \tLoss 0.06435190886259079\n",
      "57000/2862 \tLoss 0.07309596985578537\n",
      "57050/2862 \tLoss 0.05969246104359627\n",
      "57100/2862 \tLoss 0.029438091441988945\n",
      "57150/2862 \tLoss 0.06395537406206131\n",
      "57200/2862 \tLoss 0.06533442437648773\n",
      "Start Training\n",
      "57250/2862 \tLoss 0.023881299421191216\n",
      "57300/2862 \tLoss 0.07123729586601257\n",
      "57350/2862 \tLoss 0.07314471155405045\n",
      "57400/2862 \tLoss 0.04936496913433075\n",
      "57450/2862 \tLoss 0.06333830952644348\n",
      "57500/2862 \tLoss 0.08322484791278839\n",
      "57550/2862 \tLoss 0.05523941293358803\n",
      "57600/2862 \tLoss 0.03836566209793091\n",
      "57650/2862 \tLoss 0.07533946633338928\n",
      "57700/2862 \tLoss 0.07668210566043854\n",
      "57750/2862 \tLoss 0.053902894258499146\n",
      "57800/2862 \tLoss 0.08043055236339569\n",
      "57850/2862 \tLoss 0.06323343515396118\n",
      "57900/2862 \tLoss 0.17563194036483765\n",
      "57950/2862 \tLoss 0.11015309393405914\n",
      "58000/2862 \tLoss 0.041159048676490784\n",
      "58050/2862 \tLoss 0.05225970223546028\n",
      "58100/2862 \tLoss 0.08525639772415161\n",
      "58150/2862 \tLoss 0.026727883145213127\n",
      "58200/2862 \tLoss 0.06405751407146454\n",
      "58250/2862 \tLoss 0.055561695247888565\n",
      "58300/2862 \tLoss 0.14121702313423157\n",
      "58350/2862 \tLoss 0.09757117927074432\n",
      "58400/2862 \tLoss 0.05980490893125534\n",
      "58450/2862 \tLoss 0.07346230745315552\n",
      "58500/2862 \tLoss 0.10019563883543015\n",
      "58550/2862 \tLoss 0.0490482822060585\n",
      "58600/2862 \tLoss 0.05786622315645218\n",
      "58650/2862 \tLoss 0.07204322516918182\n",
      "58700/2862 \tLoss 0.05709733068943024\n",
      "58750/2862 \tLoss 0.03907709941267967\n",
      "58800/2862 \tLoss 0.058800797909498215\n",
      "58850/2862 \tLoss 0.11440030485391617\n",
      "58900/2862 \tLoss 0.0610663928091526\n",
      "58950/2862 \tLoss 0.036517780274152756\n",
      "59000/2862 \tLoss 0.12298783659934998\n",
      "59050/2862 \tLoss 0.04922882467508316\n",
      "59100/2862 \tLoss 0.0842791348695755\n",
      "59150/2862 \tLoss 0.10977354645729065\n",
      "59200/2862 \tLoss 0.08059673756361008\n",
      "59250/2862 \tLoss 0.12839967012405396\n",
      "59300/2862 \tLoss 0.05054222047328949\n",
      "59350/2862 \tLoss 0.08874727785587311\n",
      "59400/2862 \tLoss 0.10464759171009064\n",
      "59450/2862 \tLoss 0.23078495264053345\n",
      "59500/2862 \tLoss 0.12433815002441406\n",
      "59550/2862 \tLoss 0.03784504160284996\n",
      "59600/2862 \tLoss 0.09262198954820633\n",
      "59650/2862 \tLoss 0.12144053727388382\n",
      "59700/2862 \tLoss 0.044457677751779556\n",
      "59750/2862 \tLoss 0.09743866324424744\n",
      "59800/2862 \tLoss 0.11471813917160034\n",
      "59850/2862 \tLoss 0.06908190250396729\n",
      "59900/2862 \tLoss 0.03671722114086151\n",
      "59950/2862 \tLoss 0.02576637454330921\n",
      "60000/2862 \tLoss 0.02397160790860653\n",
      "60050/2862 \tLoss 0.14005345106124878\n",
      "60100/2862 \tLoss 0.05465276911854744\n",
      "Start Training\n",
      "60150/2862 \tLoss 0.08799856901168823\n",
      "60200/2862 \tLoss 0.08012907952070236\n",
      "60250/2862 \tLoss 0.10612434148788452\n",
      "60300/2862 \tLoss 0.0887497216463089\n",
      "60350/2862 \tLoss 0.05962970852851868\n",
      "60400/2862 \tLoss 0.10596820712089539\n",
      "60450/2862 \tLoss 0.04105253145098686\n",
      "60500/2862 \tLoss 0.1196361631155014\n",
      "60550/2862 \tLoss 0.06355545669794083\n",
      "60600/2862 \tLoss 0.08440747857093811\n",
      "60650/2862 \tLoss 0.07684509456157684\n",
      "60700/2862 \tLoss 0.028409115970134735\n",
      "60750/2862 \tLoss 0.05997314304113388\n",
      "60800/2862 \tLoss 0.08490224182605743\n",
      "60850/2862 \tLoss 0.09025406092405319\n",
      "60900/2862 \tLoss 0.049545496702194214\n",
      "60950/2862 \tLoss 0.08319985121488571\n",
      "61000/2862 \tLoss 0.061882536858320236\n",
      "61050/2862 \tLoss 0.04839833080768585\n",
      "61100/2862 \tLoss 0.08339080959558487\n",
      "61150/2862 \tLoss 0.09295317530632019\n",
      "61200/2862 \tLoss 0.036393120884895325\n",
      "61250/2862 \tLoss 0.05534043163061142\n",
      "61300/2862 \tLoss 0.07160727679729462\n",
      "61350/2862 \tLoss 0.029679225757718086\n",
      "61400/2862 \tLoss 0.06940556317567825\n",
      "61450/2862 \tLoss 0.07970952987670898\n",
      "61500/2862 \tLoss 0.025579791516065598\n",
      "61550/2862 \tLoss 0.08113853633403778\n",
      "61600/2862 \tLoss 0.06342564523220062\n",
      "61650/2862 \tLoss 0.11570660024881363\n",
      "61700/2862 \tLoss 0.038866735994815826\n",
      "61750/2862 \tLoss 0.06074598431587219\n",
      "61800/2862 \tLoss 0.04972349479794502\n",
      "61850/2862 \tLoss 0.068602554500103\n",
      "61900/2862 \tLoss 0.06900592893362045\n",
      "61950/2862 \tLoss 0.11317048221826553\n",
      "62000/2862 \tLoss 0.1192324087023735\n",
      "62050/2862 \tLoss 0.053701579570770264\n",
      "62100/2862 \tLoss 0.06088211387395859\n",
      "62150/2862 \tLoss 0.09865884482860565\n",
      "62200/2862 \tLoss 0.07054029405117035\n",
      "62250/2862 \tLoss 0.0337376669049263\n",
      "62300/2862 \tLoss 0.09448323398828506\n",
      "62350/2862 \tLoss 0.05587659403681755\n",
      "62400/2862 \tLoss 0.03538726270198822\n",
      "62450/2862 \tLoss 0.01784110814332962\n",
      "62500/2862 \tLoss 0.15376190841197968\n",
      "62550/2862 \tLoss 0.023109782487154007\n",
      "62600/2862 \tLoss 0.1313416212797165\n",
      "62650/2862 \tLoss 0.09996064752340317\n",
      "62700/2862 \tLoss 0.06154641509056091\n",
      "62750/2862 \tLoss 0.05576306954026222\n",
      "62800/2862 \tLoss 0.13522551953792572\n",
      "62850/2862 \tLoss 0.03435954824090004\n",
      "62900/2862 \tLoss 0.03846614062786102\n",
      "62950/2862 \tLoss 0.10264289379119873\n",
      "Start Training\n",
      "63000/2862 \tLoss 0.13076651096343994\n",
      "63050/2862 \tLoss 0.031254615634679794\n",
      "63100/2862 \tLoss 0.06227888539433479\n",
      "63150/2862 \tLoss 0.09272077679634094\n",
      "63200/2862 \tLoss 0.05784228444099426\n",
      "63250/2862 \tLoss 0.07389184832572937\n",
      "63300/2862 \tLoss 0.047093287110328674\n",
      "63350/2862 \tLoss 0.028635278344154358\n",
      "63400/2862 \tLoss 0.1052708625793457\n",
      "63450/2862 \tLoss 0.061805251985788345\n",
      "63500/2862 \tLoss 0.09521298110485077\n",
      "63550/2862 \tLoss 0.059014301747083664\n",
      "63600/2862 \tLoss 0.07625717669725418\n",
      "63650/2862 \tLoss 0.053861089050769806\n",
      "63700/2862 \tLoss 0.037145860493183136\n",
      "63750/2862 \tLoss 0.05026491731405258\n",
      "63800/2862 \tLoss 0.03713696822524071\n",
      "63850/2862 \tLoss 0.03443146497011185\n",
      "63900/2862 \tLoss 0.007031199522316456\n",
      "63950/2862 \tLoss 0.06633324921131134\n",
      "64000/2862 \tLoss 0.05164819955825806\n",
      "64050/2862 \tLoss 0.020906368270516396\n",
      "64100/2862 \tLoss 0.1258893460035324\n",
      "64150/2862 \tLoss 0.04600369185209274\n",
      "64200/2862 \tLoss 0.1624896228313446\n",
      "64250/2862 \tLoss 0.13335289061069489\n",
      "64300/2862 \tLoss 0.21828222274780273\n",
      "64350/2862 \tLoss 0.01792113669216633\n",
      "64400/2862 \tLoss 0.06867682933807373\n",
      "64450/2862 \tLoss 0.09492657333612442\n",
      "64500/2862 \tLoss 0.045095764100551605\n",
      "64550/2862 \tLoss 0.04174816980957985\n",
      "64600/2862 \tLoss 0.04607158899307251\n",
      "64650/2862 \tLoss 0.06981313228607178\n",
      "64700/2862 \tLoss 0.027992090210318565\n",
      "64750/2862 \tLoss 0.12800677120685577\n",
      "64800/2862 \tLoss 0.13051453232765198\n",
      "64850/2862 \tLoss 0.09397272765636444\n",
      "64900/2862 \tLoss 0.04312141239643097\n",
      "64950/2862 \tLoss 0.12784722447395325\n",
      "65000/2862 \tLoss 0.026438595727086067\n",
      "65050/2862 \tLoss 0.07717008888721466\n",
      "65100/2862 \tLoss 0.10730743408203125\n",
      "65150/2862 \tLoss 0.08814500272274017\n",
      "65200/2862 \tLoss 0.03527292236685753\n",
      "65250/2862 \tLoss 0.062260352075099945\n",
      "65300/2862 \tLoss 0.04035264626145363\n",
      "65350/2862 \tLoss 0.07868876308202744\n",
      "65400/2862 \tLoss 0.04125939682126045\n",
      "65450/2862 \tLoss 0.1434108167886734\n",
      "65500/2862 \tLoss 0.13364540040493011\n",
      "65550/2862 \tLoss 0.10907929390668869\n",
      "65600/2862 \tLoss 0.03436153754591942\n",
      "65650/2862 \tLoss 0.05027233809232712\n",
      "65700/2862 \tLoss 0.040905654430389404\n",
      "65750/2862 \tLoss 0.04517208784818649\n",
      "65800/2862 \tLoss 0.0422140471637249\n",
      "Start Training\n",
      "65850/2862 \tLoss 0.3025085926055908\n",
      "65900/2862 \tLoss 0.05134569853544235\n",
      "65950/2862 \tLoss 0.07957981526851654\n",
      "66000/2862 \tLoss 0.12132496386766434\n",
      "66050/2862 \tLoss 0.140719935297966\n",
      "66100/2862 \tLoss 0.055589739233255386\n",
      "66150/2862 \tLoss 0.02228889986872673\n",
      "66200/2862 \tLoss 0.012885336764156818\n",
      "66250/2862 \tLoss 0.07628022134304047\n",
      "66300/2862 \tLoss 0.07729708403348923\n",
      "66350/2862 \tLoss 0.07259084284305573\n",
      "66400/2862 \tLoss 0.026171164587140083\n",
      "66450/2862 \tLoss 0.08357478678226471\n",
      "66500/2862 \tLoss 0.018855594098567963\n",
      "66550/2862 \tLoss 0.041697561740875244\n",
      "66600/2862 \tLoss 0.11136579513549805\n",
      "66650/2862 \tLoss 0.24513784050941467\n",
      "66700/2862 \tLoss 0.11502260714769363\n",
      "66750/2862 \tLoss 0.09701649099588394\n",
      "66800/2862 \tLoss 0.043401096016168594\n",
      "66850/2862 \tLoss 0.16029299795627594\n",
      "66900/2862 \tLoss 0.04982580244541168\n",
      "66950/2862 \tLoss 0.02838972583413124\n",
      "67000/2862 \tLoss 0.047930262982845306\n",
      "67050/2862 \tLoss 0.1634255349636078\n",
      "67100/2862 \tLoss 0.017259832471609116\n",
      "67150/2862 \tLoss 0.12757152318954468\n",
      "67200/2862 \tLoss 0.12161561846733093\n",
      "67250/2862 \tLoss 0.12100499868392944\n",
      "67300/2862 \tLoss 0.04302605614066124\n",
      "67350/2862 \tLoss 0.07799425721168518\n",
      "67400/2862 \tLoss 0.05310777202248573\n",
      "67450/2862 \tLoss 0.05423615500330925\n",
      "67500/2862 \tLoss 0.05509501323103905\n",
      "67550/2862 \tLoss 0.16153785586357117\n",
      "67600/2862 \tLoss 0.04563744366168976\n",
      "67650/2862 \tLoss 0.04317345470190048\n",
      "67700/2862 \tLoss 0.03182896599173546\n",
      "67750/2862 \tLoss 0.18222960829734802\n",
      "67800/2862 \tLoss 0.08139584958553314\n",
      "67850/2862 \tLoss 0.172059565782547\n",
      "67900/2862 \tLoss 0.07210133969783783\n",
      "67950/2862 \tLoss 0.028191134333610535\n",
      "68000/2862 \tLoss 0.03816775977611542\n",
      "68050/2862 \tLoss 0.11630942672491074\n",
      "68100/2862 \tLoss 0.021737568080425262\n",
      "68150/2862 \tLoss 0.09918458759784698\n",
      "68200/2862 \tLoss 0.08580385893583298\n",
      "68250/2862 \tLoss 0.055142275989055634\n",
      "68300/2862 \tLoss 0.13851094245910645\n",
      "68350/2862 \tLoss 0.0791269987821579\n",
      "68400/2862 \tLoss 0.10242709517478943\n",
      "68450/2862 \tLoss 0.09196169674396515\n",
      "68500/2862 \tLoss 0.05262232571840286\n",
      "68550/2862 \tLoss 0.07950951159000397\n",
      "68600/2862 \tLoss 0.022523103281855583\n",
      "68650/2862 \tLoss 0.13084295392036438\n",
      "Start Training\n",
      "68700/2862 \tLoss 0.09260760992765427\n",
      "68750/2862 \tLoss 0.04056191444396973\n",
      "68800/2862 \tLoss 0.116212859749794\n",
      "68850/2862 \tLoss 0.031981371343135834\n",
      "68900/2862 \tLoss 0.1383409947156906\n",
      "68950/2862 \tLoss 0.04562752693891525\n",
      "69000/2862 \tLoss 0.13024497032165527\n",
      "69050/2862 \tLoss 0.08510352671146393\n",
      "69100/2862 \tLoss 0.21528762578964233\n",
      "69150/2862 \tLoss 0.08582724630832672\n",
      "69200/2862 \tLoss 0.06099218502640724\n",
      "69250/2862 \tLoss 0.008846886456012726\n",
      "69300/2862 \tLoss 0.20051199197769165\n",
      "69350/2862 \tLoss 0.016575362533330917\n",
      "69400/2862 \tLoss 0.07050561159849167\n",
      "69450/2862 \tLoss 0.07193315029144287\n",
      "69500/2862 \tLoss 0.060698989778757095\n",
      "69550/2862 \tLoss 0.06171156466007233\n",
      "69600/2862 \tLoss 0.016940975561738014\n",
      "69650/2862 \tLoss 0.02070442959666252\n",
      "69700/2862 \tLoss 0.025894295424222946\n",
      "69750/2862 \tLoss 0.0676611140370369\n",
      "69800/2862 \tLoss 0.10125026106834412\n",
      "69850/2862 \tLoss 0.020554615184664726\n",
      "69900/2862 \tLoss 0.08549356460571289\n",
      "69950/2862 \tLoss 0.031868476420640945\n",
      "70000/2862 \tLoss 0.13201972842216492\n",
      "70050/2862 \tLoss 0.07458797097206116\n",
      "70100/2862 \tLoss 0.07603111118078232\n",
      "70150/2862 \tLoss 0.08978290855884552\n",
      "70200/2862 \tLoss 0.05233792960643768\n",
      "70250/2862 \tLoss 0.06890611350536346\n",
      "70300/2862 \tLoss 0.05954447388648987\n",
      "70350/2862 \tLoss 0.022787893190979958\n",
      "70400/2862 \tLoss 0.03639426454901695\n",
      "70450/2862 \tLoss 0.05865929275751114\n",
      "70500/2862 \tLoss 0.13609841465950012\n",
      "70550/2862 \tLoss 0.051045551896095276\n",
      "70600/2862 \tLoss 0.027504147961735725\n",
      "70650/2862 \tLoss 0.08209459483623505\n",
      "70700/2862 \tLoss 0.15640655159950256\n",
      "70750/2862 \tLoss 0.08981126546859741\n",
      "70800/2862 \tLoss 0.03247900679707527\n",
      "70850/2862 \tLoss 0.1277131885290146\n",
      "70900/2862 \tLoss 0.0919228196144104\n",
      "70950/2862 \tLoss 0.05435718595981598\n",
      "71000/2862 \tLoss 0.06566338986158371\n",
      "71050/2862 \tLoss 0.04219649359583855\n",
      "71100/2862 \tLoss 0.10678219050168991\n",
      "71150/2862 \tLoss 0.14918279647827148\n",
      "71200/2862 \tLoss 0.011610157787799835\n",
      "71250/2862 \tLoss 0.0716354176402092\n",
      "71300/2862 \tLoss 0.0774187445640564\n",
      "71350/2862 \tLoss 0.05167852342128754\n",
      "71400/2862 \tLoss 0.0827847495675087\n",
      "71450/2862 \tLoss 0.0813848227262497\n",
      "71500/2862 \tLoss 0.06025193631649017\n",
      "Start Training\n",
      "71550/2862 \tLoss 0.14801359176635742\n",
      "71600/2862 \tLoss 0.04673303663730621\n",
      "71650/2862 \tLoss 0.06621922552585602\n",
      "71700/2862 \tLoss 0.13908998668193817\n",
      "71750/2862 \tLoss 0.05779184028506279\n",
      "71800/2862 \tLoss 0.03097868710756302\n",
      "71850/2862 \tLoss 0.035060442984104156\n",
      "71900/2862 \tLoss 0.05884441360831261\n",
      "71950/2862 \tLoss 0.07754258811473846\n",
      "72000/2862 \tLoss 0.04487626999616623\n",
      "72050/2862 \tLoss 0.019163772463798523\n",
      "72100/2862 \tLoss 0.051522038877010345\n",
      "72150/2862 \tLoss 0.04907030612230301\n",
      "72200/2862 \tLoss 0.07197772711515427\n",
      "72250/2862 \tLoss 0.026094701141119003\n",
      "72300/2862 \tLoss 0.03924459591507912\n",
      "72350/2862 \tLoss 0.06946489214897156\n",
      "72400/2862 \tLoss 0.09655609726905823\n",
      "72450/2862 \tLoss 0.06535998731851578\n",
      "72500/2862 \tLoss 0.018893886357545853\n",
      "72550/2862 \tLoss 0.07847818732261658\n",
      "72600/2862 \tLoss 0.04014819860458374\n",
      "72650/2862 \tLoss 0.09301638603210449\n",
      "72700/2862 \tLoss 0.08370375633239746\n",
      "72750/2862 \tLoss 0.09008071571588516\n",
      "72800/2862 \tLoss 0.008947238326072693\n",
      "72850/2862 \tLoss 0.18829220533370972\n",
      "72900/2862 \tLoss 0.0943368524312973\n",
      "72950/2862 \tLoss 0.11162092536687851\n",
      "73000/2862 \tLoss 0.12792792916297913\n",
      "73050/2862 \tLoss 0.10646158456802368\n",
      "73100/2862 \tLoss 0.07618765532970428\n",
      "73150/2862 \tLoss 0.05365185812115669\n",
      "73200/2862 \tLoss 0.08700527250766754\n",
      "73250/2862 \tLoss 0.023551052436232567\n",
      "73300/2862 \tLoss 0.035023681819438934\n",
      "73350/2862 \tLoss 0.0689818486571312\n",
      "73400/2862 \tLoss 0.024378083646297455\n",
      "73450/2862 \tLoss 0.08219882845878601\n",
      "73500/2862 \tLoss 0.0876314789056778\n",
      "73550/2862 \tLoss 0.09454874694347382\n",
      "73600/2862 \tLoss 0.12893147766590118\n",
      "73650/2862 \tLoss 0.05936586484313011\n",
      "73700/2862 \tLoss 0.10906760394573212\n",
      "73750/2862 \tLoss 0.04070984572172165\n",
      "73800/2862 \tLoss 0.028922783210873604\n",
      "73850/2862 \tLoss 0.1684880405664444\n",
      "73900/2862 \tLoss 0.1048777624964714\n",
      "73950/2862 \tLoss 0.11270548403263092\n",
      "74000/2862 \tLoss 0.017189478501677513\n",
      "74050/2862 \tLoss 0.1253966987133026\n",
      "74100/2862 \tLoss 0.06594705581665039\n",
      "74150/2862 \tLoss 0.06941725313663483\n",
      "74200/2862 \tLoss 0.16773878037929535\n",
      "74250/2862 \tLoss 0.05084247887134552\n",
      "74300/2862 \tLoss 0.040590859949588776\n",
      "74350/2862 \tLoss 0.006608741357922554\n",
      "74400/2862 \tLoss 0.12313740700483322\n",
      "Start Training\n",
      "74450/2862 \tLoss 0.024464502930641174\n",
      "74500/2862 \tLoss 0.08570878952741623\n",
      "74550/2862 \tLoss 0.038114629685878754\n",
      "74600/2862 \tLoss 0.07604295015335083\n",
      "74650/2862 \tLoss 0.16791348159313202\n",
      "74700/2862 \tLoss 0.10187456011772156\n",
      "74750/2862 \tLoss 0.07286685705184937\n",
      "74800/2862 \tLoss 0.03299232944846153\n",
      "74850/2862 \tLoss 0.16016799211502075\n",
      "74900/2862 \tLoss 0.05933600664138794\n",
      "74950/2862 \tLoss 0.07798656821250916\n",
      "75000/2862 \tLoss 0.0215950645506382\n",
      "75050/2862 \tLoss 0.05565407872200012\n",
      "75100/2862 \tLoss 0.0638427808880806\n",
      "75150/2862 \tLoss 0.03532668203115463\n",
      "75200/2862 \tLoss 0.04735960811376572\n",
      "75250/2862 \tLoss 0.022135496139526367\n",
      "75300/2862 \tLoss 0.06012754887342453\n",
      "75350/2862 \tLoss 0.0504930317401886\n",
      "75400/2862 \tLoss 0.11437519639730453\n",
      "75450/2862 \tLoss 0.0989885926246643\n",
      "75500/2862 \tLoss 0.07655560970306396\n",
      "75550/2862 \tLoss 0.023813260719180107\n",
      "75600/2862 \tLoss 0.07108107954263687\n",
      "75650/2862 \tLoss 0.09725113213062286\n",
      "75700/2862 \tLoss 0.021682150661945343\n",
      "75750/2862 \tLoss 0.0484195202589035\n",
      "75800/2862 \tLoss 0.04558856412768364\n",
      "75850/2862 \tLoss 0.11177325248718262\n",
      "75900/2862 \tLoss 0.037922076880931854\n",
      "75950/2862 \tLoss 0.05931677669286728\n",
      "76000/2862 \tLoss 0.06632359325885773\n",
      "76050/2862 \tLoss 0.01665060967206955\n",
      "76100/2862 \tLoss 0.08188098669052124\n",
      "76150/2862 \tLoss 0.039141226559877396\n",
      "76200/2862 \tLoss 0.010157395154237747\n",
      "76250/2862 \tLoss 0.04956865310668945\n",
      "76300/2862 \tLoss 0.0711253434419632\n",
      "76350/2862 \tLoss 0.03792746737599373\n",
      "76400/2862 \tLoss 0.0340355820953846\n",
      "76450/2862 \tLoss 0.019574014469981194\n",
      "76500/2862 \tLoss 0.07904697954654694\n",
      "76550/2862 \tLoss 0.036960117518901825\n",
      "76600/2862 \tLoss 0.056542009115219116\n",
      "76650/2862 \tLoss 0.07391789555549622\n",
      "76700/2862 \tLoss 0.019533343613147736\n",
      "76750/2862 \tLoss 0.025548622012138367\n",
      "76800/2862 \tLoss 0.018775956705212593\n",
      "76850/2862 \tLoss 0.05480574816465378\n",
      "76900/2862 \tLoss 0.04541352391242981\n",
      "76950/2862 \tLoss 0.03337810933589935\n",
      "77000/2862 \tLoss 0.018825531005859375\n",
      "77050/2862 \tLoss 0.0974205881357193\n",
      "77100/2862 \tLoss 0.011165710166096687\n",
      "77150/2862 \tLoss 0.019962023943662643\n",
      "77200/2862 \tLoss 0.12668359279632568\n",
      "77250/2862 \tLoss 0.041722964495420456\n",
      "Start Training\n",
      "77300/2862 \tLoss 0.07808654010295868\n",
      "77350/2862 \tLoss 0.026985863223671913\n",
      "77400/2862 \tLoss 0.0741828978061676\n",
      "77450/2862 \tLoss 0.14120152592658997\n",
      "77500/2862 \tLoss 0.01880718022584915\n",
      "77550/2862 \tLoss 0.02904239110648632\n",
      "77600/2862 \tLoss 0.03281233832240105\n",
      "77650/2862 \tLoss 0.0262269489467144\n",
      "77700/2862 \tLoss 0.05278564989566803\n",
      "77750/2862 \tLoss 0.07801242917776108\n",
      "77800/2862 \tLoss 0.03604523092508316\n",
      "77850/2862 \tLoss 0.10287302732467651\n",
      "77900/2862 \tLoss 0.03824532777070999\n",
      "77950/2862 \tLoss 0.07880369573831558\n",
      "78000/2862 \tLoss 0.058899715542793274\n",
      "78050/2862 \tLoss 0.06327373534440994\n",
      "78100/2862 \tLoss 0.054279908537864685\n",
      "78150/2862 \tLoss 0.14767488837242126\n",
      "78200/2862 \tLoss 0.036851558834314346\n",
      "78250/2862 \tLoss 0.014498429372906685\n",
      "78300/2862 \tLoss 0.02913491614162922\n",
      "78350/2862 \tLoss 0.007693963125348091\n",
      "78400/2862 \tLoss 0.10768470913171768\n",
      "78450/2862 \tLoss 0.03916193172335625\n",
      "78500/2862 \tLoss 0.06761732697486877\n",
      "78550/2862 \tLoss 0.04135212302207947\n",
      "78600/2862 \tLoss 0.07455350458621979\n",
      "78650/2862 \tLoss 0.08141950517892838\n",
      "78700/2862 \tLoss 0.07019040733575821\n",
      "78750/2862 \tLoss 0.04168436676263809\n",
      "78800/2862 \tLoss 0.099472776055336\n",
      "78850/2862 \tLoss 0.07102794945240021\n",
      "78900/2862 \tLoss 0.03206195682287216\n",
      "78950/2862 \tLoss 0.06362978368997574\n",
      "79000/2862 \tLoss 0.06823096424341202\n",
      "79050/2862 \tLoss 0.0989028811454773\n",
      "79100/2862 \tLoss 0.07458636164665222\n",
      "79150/2862 \tLoss 0.0489267073571682\n",
      "79200/2862 \tLoss 0.15747740864753723\n",
      "79250/2862 \tLoss 0.045374445617198944\n",
      "79300/2862 \tLoss 0.02554001286625862\n",
      "79350/2862 \tLoss 0.014846139587461948\n",
      "79400/2862 \tLoss 0.09373029321432114\n",
      "79450/2862 \tLoss 0.14245277643203735\n",
      "79500/2862 \tLoss 0.01618025451898575\n",
      "79550/2862 \tLoss 0.11633341014385223\n",
      "79600/2862 \tLoss 0.07991071045398712\n",
      "79650/2862 \tLoss 0.05661247670650482\n",
      "79700/2862 \tLoss 0.10356760770082474\n",
      "79750/2862 \tLoss 0.06526895612478256\n",
      "79800/2862 \tLoss 0.047571588307619095\n",
      "79850/2862 \tLoss 0.06034556403756142\n",
      "79900/2862 \tLoss 0.04325251653790474\n",
      "79950/2862 \tLoss 0.018990788608789444\n",
      "80000/2862 \tLoss 0.04612527787685394\n",
      "80050/2862 \tLoss 0.05039188638329506\n",
      "80100/2862 \tLoss 0.10801321268081665\n",
      "Start Training\n",
      "80150/2862 \tLoss 0.1304996758699417\n",
      "80200/2862 \tLoss 0.04505130276083946\n",
      "80250/2862 \tLoss 0.060280896723270416\n",
      "80300/2862 \tLoss 0.10216633230447769\n",
      "80350/2862 \tLoss 0.07257138192653656\n",
      "80400/2862 \tLoss 0.04321549832820892\n",
      "80450/2862 \tLoss 0.07478398084640503\n",
      "80500/2862 \tLoss 0.02432098239660263\n",
      "80550/2862 \tLoss 0.04851675033569336\n",
      "80600/2862 \tLoss 0.01527266763150692\n",
      "80650/2862 \tLoss 0.03490544483065605\n",
      "80700/2862 \tLoss 0.02279282733798027\n",
      "80750/2862 \tLoss 0.06609489768743515\n",
      "80800/2862 \tLoss 0.04379665479063988\n",
      "80850/2862 \tLoss 0.02864534966647625\n",
      "80900/2862 \tLoss 0.08002147078514099\n",
      "80950/2862 \tLoss 0.04290255159139633\n",
      "81000/2862 \tLoss 0.08419054746627808\n",
      " 30.43%\r"
     ]
    }
   ],
   "source": [
    "for i in range(10,40):\n",
    "    val_res = validate(val_loader, model, criterion)\n",
    "    validation_logs(i, val_res)\n",
    "    loss = train(\n",
    "                    train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    i,\n",
    "                    on_iteration=on_iteration_logs,\n",
    "                )\n",
    "    logger.add_scalar(\"Loss/Avg_Train\", loss, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, child in enumerate(model.children()):\n",
    "    if i > 3:\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad=True\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=INITIAL_LR*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40,80):\n",
    "    val_res = validate(val_loader, model, criterion)\n",
    "    validation_logs(i, val_res)\n",
    "    loss = train(\n",
    "                    train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    i,\n",
    "                    on_iteration=on_iteration_logs,\n",
    "                )\n",
    "    logger.add_scalar(\"Loss/Avg_Train\", loss, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
