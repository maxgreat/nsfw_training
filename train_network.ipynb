{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "MEAN_PREPRO = [0.485, 0.456, 0.406]\n",
    "STD_PREPRO = [0.229, 0.224, 0.225]\n",
    "RESIZE_PREPRO = 256,256\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TRAIN_SHUFFLE = True\n",
    "TRAIN_NUM_WORKERS = 8\n",
    "TRAIN_PIN_MEMORY = True\n",
    "\n",
    "VAL_BATCH_SIZE = 512\n",
    "VAL_SHUFFLE = False\n",
    "VAL_NUM_WORKERS = 4\n",
    "VAL_PIN_MEMORY = True\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "DEVICE_ID = 1\n",
    "\n",
    "DATA_DIR = '/data/porn/binary/'\n",
    "TRAINSET_ROOT = f'{DATA_DIR}/train'\n",
    "TESTSET_ROOT = f'{DATA_DIR}/test'\n",
    "\n",
    "TENSORBOARD_DIR = '/data/porn/tensorboard/resnet50-binary/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "        mean=MEAN_PREPRO, std=STD_PREPRO\n",
    "    )\n",
    "prepro = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "prepro_val = transforms.Compose(\n",
    "    [transforms.Resize(RESIZE_PREPRO), transforms.ToTensor(), normalize]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(TRAINSET_ROOT, transform=prepro)\n",
    "val_dataset = ImageFolder(TESTSET_ROOT, transform=prepro_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 183210\n",
      "    Root location: /data/porn/binary//train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(256, 256), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "['nsfw', 'safe']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=TRAIN_SHUFFLE, num_workers=TRAIN_NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=VAL_SHUFFLE, num_workers=VAL_NUM_WORKERS, pin_memory=VAL_PIN_MEMORY, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(2048, len(train_dataset.classes))\n",
    "DEVICE = f\"cuda:{DEVICE_ID}\"\n",
    "model = model.to(DEVICE)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=INITIAL_LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(classes, cm):\n",
    "    fig, ax = plt.subplots(figsize=(len(classes),len(classes)))  \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax,fmt=\"d\")\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels') \n",
    "    ax.set_title('Confusion Matrix') \n",
    "    ax.xaxis.set_ticklabels(classes,rotation=90)\n",
    "    ax.yaxis.set_ticklabels(classes,rotation=0)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    on_iteration=None,\n",
    "):\n",
    "    model = model.train()\n",
    "    end = time.time()\n",
    "    print(\"Start Training\")\n",
    "    avg_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        print(f\"{i/len(train_loader) * 100 : 2.2f}%\", end=\"\\r\")\n",
    "        iteration_time = time.time()\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if on_iteration is not None:\n",
    "            on_iteration(iteration=i+epoch*len(train_loader), loss=loss, y_pred=outputs, y_true=labels)     \n",
    "    return avg_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, \n",
    "             model, \n",
    "             criterion,\n",
    "             print_freq=1000):\n",
    "    model = model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    avg_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(val_loader):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            avg_loss += criterion(outputs, labels).item()\n",
    "        _, indices = torch.max(outputs.cpu(), 1)\n",
    "        y_pred.append(indices)\n",
    "        y_true.append(labels.cpu().clone())\n",
    "    return avg_loss/len(val_loader), torch.cat(y_pred), torch.cat(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_porn_detection(epoch, y_pred, y_true):\n",
    "    ones = torch.ones(len(y_pred))\n",
    "    p_pred = (y_pred == 1) | (y_pred == 3)\n",
    "    p_truth = (y_true == 1) | (y_true == 3)\n",
    "    logger.add_pr_curve(\"Eval/Prec_recall\", p_truth, p_pred, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_logs(epoch, loss_avg, y_pred, y_true):\n",
    "    logger.add_scalar(\"Loss/Avg_Val\", loss_avg, epoch)\n",
    "    logger.add_scalar(\"Eval/Precision\", (y_pred==y_true).sum()/float(len(y_pred)), epoch)\n",
    "    compute_porn_detection(epoch, y_pred, y_true)\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    cm_image = plot_cm(train_dataset.classes, cm)\n",
    "    logger.add_figure(\"Eval/Confusion Matrix\", cm_image, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_iteration_logs(iteration, loss, y_pred, y_true):\n",
    "    l = loss.item()\n",
    "    if iteration%50 == 0:\n",
    "        logger.add_scalar(\"Loss/Train\", l, iteration)\n",
    "        print(\n",
    "                f\"{iteration}/{len(train_loader)} \\t\"\n",
    "                f\"Loss {l}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = SummaryWriter(TENSORBOARD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "0/1431 \tLoss 0.5941365957260132\n",
      "50/1431 \tLoss 0.4584013819694519\n",
      "100/1431 \tLoss 0.406158447265625\n",
      "150/1431 \tLoss 0.3455604016780853\n",
      "200/1431 \tLoss 0.3352430760860443\n",
      "250/1431 \tLoss 0.30484941601753235\n",
      "300/1431 \tLoss 0.37025219202041626\n",
      "350/1431 \tLoss 0.24612148106098175\n",
      "400/1431 \tLoss 0.25426894426345825\n",
      "450/1431 \tLoss 0.193686842918396\n",
      "500/1431 \tLoss 0.2738170623779297\n",
      "550/1431 \tLoss 0.27412036061286926\n",
      "600/1431 \tLoss 0.23882710933685303\n",
      "650/1431 \tLoss 0.2792549431324005\n",
      "700/1431 \tLoss 0.26615047454833984\n",
      "750/1431 \tLoss 0.28119412064552307\n",
      "800/1431 \tLoss 0.26657208800315857\n",
      "850/1431 \tLoss 0.1947667896747589\n",
      "900/1431 \tLoss 0.2480231076478958\n",
      "950/1431 \tLoss 0.23323087394237518\n",
      "1000/1431 \tLoss 0.26831427216529846\n",
      "1050/1431 \tLoss 0.22298309206962585\n",
      "1100/1431 \tLoss 0.2509477138519287\n",
      "1150/1431 \tLoss 0.2654440402984619\n",
      "1200/1431 \tLoss 0.27652451395988464\n",
      "1250/1431 \tLoss 0.24988488852977753\n",
      "1300/1431 \tLoss 0.22201591730117798\n",
      "1350/1431 \tLoss 0.23847956955432892\n",
      "1400/1431 \tLoss 0.3244387209415436\n",
      "Start Training\n",
      "1450/1431 \tLoss 0.274845153093338\n",
      "1500/1431 \tLoss 0.16820810735225677\n",
      "1550/1431 \tLoss 0.2805669605731964\n",
      "1600/1431 \tLoss 0.20149238407611847\n",
      "1650/1431 \tLoss 0.25948283076286316\n",
      "1700/1431 \tLoss 0.25971487164497375\n",
      "1750/1431 \tLoss 0.19070547819137573\n",
      "1800/1431 \tLoss 0.2164805829524994\n",
      "1850/1431 \tLoss 0.22058311104774475\n",
      "1900/1431 \tLoss 0.2853012681007385\n",
      "1950/1431 \tLoss 0.17300096154212952\n",
      "2000/1431 \tLoss 0.23019278049468994\n",
      "2050/1431 \tLoss 0.20700031518936157\n",
      "2100/1431 \tLoss 0.26763519644737244\n",
      "2150/1431 \tLoss 0.278281569480896\n",
      "2200/1431 \tLoss 0.2543289363384247\n",
      "2250/1431 \tLoss 0.24734735488891602\n",
      "2300/1431 \tLoss 0.23093058168888092\n",
      "2350/1431 \tLoss 0.32599925994873047\n",
      "2400/1431 \tLoss 0.2670666575431824\n",
      "2450/1431 \tLoss 0.16336584091186523\n",
      "2500/1431 \tLoss 0.24335888028144836\n",
      "2550/1431 \tLoss 0.18762998282909393\n",
      "2600/1431 \tLoss 0.2063569873571396\n",
      "2650/1431 \tLoss 0.24260587990283966\n",
      "2700/1431 \tLoss 0.1691664457321167\n",
      "2750/1431 \tLoss 0.2326630800962448\n",
      "2800/1431 \tLoss 0.17867498099803925\n",
      "2850/1431 \tLoss 0.1891351044178009\n",
      "Start Training\n",
      "2900/1431 \tLoss 0.27157267928123474\n",
      "2950/1431 \tLoss 0.29576995968818665\n",
      "3000/1431 \tLoss 0.157853364944458\n",
      "3050/1431 \tLoss 0.2198260873556137\n",
      "3100/1431 \tLoss 0.22985397279262543\n",
      "3150/1431 \tLoss 0.2638421356678009\n",
      "3200/1431 \tLoss 0.2620678246021271\n",
      "3250/1431 \tLoss 0.27474018931388855\n",
      "3300/1431 \tLoss 0.2016693502664566\n",
      "3350/1431 \tLoss 0.2003863900899887\n",
      "3400/1431 \tLoss 0.1934516429901123\n",
      "3450/1431 \tLoss 0.21331074833869934\n",
      "3500/1431 \tLoss 0.1612745076417923\n",
      "3550/1431 \tLoss 0.182210773229599\n",
      "3600/1431 \tLoss 0.22855737805366516\n",
      "3650/1431 \tLoss 0.15320701897144318\n",
      "3700/1431 \tLoss 0.19258396327495575\n",
      "3750/1431 \tLoss 0.31378546357154846\n",
      "3800/1431 \tLoss 0.1746804267168045\n",
      "3850/1431 \tLoss 0.21203352510929108\n",
      "3900/1431 \tLoss 0.2532760202884674\n",
      "3950/1431 \tLoss 0.16380155086517334\n",
      "4000/1431 \tLoss 0.17321056127548218\n",
      "4050/1431 \tLoss 0.1846388429403305\n",
      "4100/1431 \tLoss 0.2417481243610382\n",
      "4150/1431 \tLoss 0.21960799396038055\n",
      "4200/1431 \tLoss 0.24614277482032776\n",
      "4250/1431 \tLoss 0.19997698068618774\n",
      "Start Training\n",
      "4300/1431 \tLoss 0.09684614837169647\n",
      "4350/1431 \tLoss 0.2170301228761673\n",
      "4400/1431 \tLoss 0.17587898671627045\n",
      "4450/1431 \tLoss 0.21570883691310883\n",
      "4500/1431 \tLoss 0.17755378782749176\n",
      "4550/1431 \tLoss 0.19002069532871246\n",
      "4600/1431 \tLoss 0.20062406361103058\n",
      "4650/1431 \tLoss 0.2641458511352539\n",
      "4700/1431 \tLoss 0.2332768440246582\n",
      "4750/1431 \tLoss 0.3930261731147766\n",
      "4800/1431 \tLoss 0.14558760821819305\n",
      "4850/1431 \tLoss 0.19843997061252594\n",
      "4900/1431 \tLoss 0.1496986448764801\n",
      "4950/1431 \tLoss 0.18667003512382507\n",
      "5000/1431 \tLoss 0.22637571394443512\n",
      "5050/1431 \tLoss 0.29488030076026917\n",
      "5100/1431 \tLoss 0.23858606815338135\n",
      "5150/1431 \tLoss 0.2406909465789795\n",
      "5200/1431 \tLoss 0.3233863115310669\n",
      "5250/1431 \tLoss 0.21553358435630798\n",
      "5300/1431 \tLoss 0.24389883875846863\n",
      "5350/1431 \tLoss 0.3237144351005554\n",
      "5400/1431 \tLoss 0.25515714287757874\n",
      "5450/1431 \tLoss 0.18073929846286774\n",
      "5500/1431 \tLoss 0.29851800203323364\n",
      "5550/1431 \tLoss 0.24165795743465424\n",
      "5600/1431 \tLoss 0.2006216198205948\n",
      "5650/1431 \tLoss 0.26528269052505493\n",
      "5700/1431 \tLoss 0.182509645819664\n",
      "Start Training\n",
      "5750/1431 \tLoss 0.19635699689388275\n",
      "5800/1431 \tLoss 0.2944158911705017\n",
      "5850/1431 \tLoss 0.22506003081798553\n",
      "5900/1431 \tLoss 0.2553929388523102\n",
      "5950/1431 \tLoss 0.21495017409324646\n",
      "6000/1431 \tLoss 0.25215041637420654\n",
      "6050/1431 \tLoss 0.16434234380722046\n",
      "6100/1431 \tLoss 0.2521507740020752\n",
      "6150/1431 \tLoss 0.22835655510425568\n",
      "6200/1431 \tLoss 0.3075357675552368\n",
      "6250/1431 \tLoss 0.16976884007453918\n",
      "6300/1431 \tLoss 0.14238882064819336\n",
      "6350/1431 \tLoss 0.1268625110387802\n",
      "6400/1431 \tLoss 0.2345959097146988\n",
      "6450/1431 \tLoss 0.19073380529880524\n",
      "6500/1431 \tLoss 0.15381288528442383\n",
      "6550/1431 \tLoss 0.2551881670951843\n",
      "6600/1431 \tLoss 0.24423658847808838\n",
      "6650/1431 \tLoss 0.12882551550865173\n",
      "6700/1431 \tLoss 0.25040343403816223\n",
      "6750/1431 \tLoss 0.1840202361345291\n",
      "6800/1431 \tLoss 0.1950322836637497\n",
      "6850/1431 \tLoss 0.1535613089799881\n",
      "6900/1431 \tLoss 0.23367416858673096\n",
      "6950/1431 \tLoss 0.19861915707588196\n",
      "7000/1431 \tLoss 0.1983555108308792\n",
      "7050/1431 \tLoss 0.23246726393699646\n",
      "7100/1431 \tLoss 0.18607787787914276\n",
      "7150/1431 \tLoss 0.23828735947608948\n",
      "Start Training\n",
      "7200/1431 \tLoss 0.2525743842124939\n",
      "7250/1431 \tLoss 0.22657056152820587\n",
      "7300/1431 \tLoss 0.1397896260023117\n",
      "7350/1431 \tLoss 0.19157131016254425\n",
      "7400/1431 \tLoss 0.17322313785552979\n",
      "7450/1431 \tLoss 0.18899542093276978\n",
      "7500/1431 \tLoss 0.2017490267753601\n",
      "7550/1431 \tLoss 0.13257479667663574\n",
      "7600/1431 \tLoss 0.2525846064090729\n",
      "7650/1431 \tLoss 0.28000152111053467\n",
      "7700/1431 \tLoss 0.19495569169521332\n",
      "7750/1431 \tLoss 0.18549573421478271\n",
      "7800/1431 \tLoss 0.20926260948181152\n",
      "7850/1431 \tLoss 0.21662072837352753\n",
      "7900/1431 \tLoss 0.19778494536876678\n",
      "7950/1431 \tLoss 0.26919111609458923\n",
      "8000/1431 \tLoss 0.18885277211666107\n",
      "8050/1431 \tLoss 0.2430722713470459\n",
      "8100/1431 \tLoss 0.19677641987800598\n",
      "8150/1431 \tLoss 0.13571614027023315\n",
      "8200/1431 \tLoss 0.14446280896663666\n",
      "8250/1431 \tLoss 0.2082541584968567\n",
      "8300/1431 \tLoss 0.1882854700088501\n",
      "8350/1431 \tLoss 0.23415148258209229\n",
      "8400/1431 \tLoss 0.15296047925949097\n",
      "8450/1431 \tLoss 0.23315925896167755\n",
      "8500/1431 \tLoss 0.19611774384975433\n",
      "8550/1431 \tLoss 0.16065216064453125\n",
      "Start Training\n",
      "8600/1431 \tLoss 0.2890821695327759\n",
      "8650/1431 \tLoss 0.15921348333358765\n",
      "8700/1431 \tLoss 0.21070337295532227\n",
      "8750/1431 \tLoss 0.19061130285263062\n",
      "8800/1431 \tLoss 0.20705994963645935\n",
      "8850/1431 \tLoss 0.2442362755537033\n",
      "8900/1431 \tLoss 0.14907324314117432\n",
      "8950/1431 \tLoss 0.22647589445114136\n",
      "9000/1431 \tLoss 0.21506989002227783\n",
      "9050/1431 \tLoss 0.1602184772491455\n",
      "9100/1431 \tLoss 0.29951900243759155\n",
      "9150/1431 \tLoss 0.2159404456615448\n",
      "9200/1431 \tLoss 0.17355914413928986\n",
      "9250/1431 \tLoss 0.1686386913061142\n",
      "9300/1431 \tLoss 0.18629172444343567\n",
      "9350/1431 \tLoss 0.28143641352653503\n",
      "9400/1431 \tLoss 0.1974082589149475\n",
      "9450/1431 \tLoss 0.1534929871559143\n",
      "9500/1431 \tLoss 0.22322040796279907\n",
      "9550/1431 \tLoss 0.16443197429180145\n",
      "9600/1431 \tLoss 0.26926693320274353\n",
      "9650/1431 \tLoss 0.35139426589012146\n",
      "9700/1431 \tLoss 0.30185917019844055\n",
      "9750/1431 \tLoss 0.2606172561645508\n",
      "9800/1431 \tLoss 0.23028923571109772\n",
      "9850/1431 \tLoss 0.24126161634922028\n",
      "9900/1431 \tLoss 0.3082559406757355\n",
      "9950/1431 \tLoss 0.15977323055267334\n",
      "10000/1431 \tLoss 0.23085293173789978\n",
      "Start Training\n",
      "10050/1431 \tLoss 0.2015887200832367\n",
      "10100/1431 \tLoss 0.16393190622329712\n",
      "10150/1431 \tLoss 0.16951632499694824\n",
      "10200/1431 \tLoss 0.20297127962112427\n",
      "10250/1431 \tLoss 0.1458669751882553\n",
      "10300/1431 \tLoss 0.1877562403678894\n",
      "10350/1431 \tLoss 0.178107351064682\n",
      "10400/1431 \tLoss 0.21685081720352173\n",
      "10450/1431 \tLoss 0.3021185100078583\n",
      "10500/1431 \tLoss 0.14355601370334625\n",
      "10550/1431 \tLoss 0.23022060096263885\n",
      "10600/1431 \tLoss 0.21055428683757782\n",
      "10650/1431 \tLoss 0.19669558107852936\n",
      "10700/1431 \tLoss 0.25498124957084656\n",
      "10750/1431 \tLoss 0.26276668906211853\n",
      "10800/1431 \tLoss 0.2653001546859741\n",
      "10850/1431 \tLoss 0.24300600588321686\n",
      "10900/1431 \tLoss 0.25561386346817017\n",
      "10950/1431 \tLoss 0.22351272404193878\n",
      "11000/1431 \tLoss 0.2055232673883438\n",
      "11050/1431 \tLoss 0.26046857237815857\n",
      "11100/1431 \tLoss 0.1611255705356598\n",
      "11150/1431 \tLoss 0.19910380244255066\n",
      "11200/1431 \tLoss 0.16034166514873505\n",
      "11250/1431 \tLoss 0.0962725430727005\n",
      "11300/1431 \tLoss 0.12132419645786285\n",
      "11350/1431 \tLoss 0.2732630968093872\n",
      "11400/1431 \tLoss 0.20669502019882202\n",
      "Start Training\n",
      "11450/1431 \tLoss 0.19148848950862885\n",
      "11500/1431 \tLoss 0.251632422208786\n",
      "11550/1431 \tLoss 0.18632137775421143\n",
      "11600/1431 \tLoss 0.24739781022071838\n",
      "11650/1431 \tLoss 0.2378840297460556\n",
      "11700/1431 \tLoss 0.17113494873046875\n",
      "11750/1431 \tLoss 0.3428792357444763\n",
      "11800/1431 \tLoss 0.19540748000144958\n",
      "11850/1431 \tLoss 0.26512017846107483\n",
      "11900/1431 \tLoss 0.2585607171058655\n",
      "11950/1431 \tLoss 0.14012931287288666\n",
      "12000/1431 \tLoss 0.1544693410396576\n",
      "12050/1431 \tLoss 0.21763700246810913\n",
      "12100/1431 \tLoss 0.17230170965194702\n",
      "12150/1431 \tLoss 0.23614779114723206\n",
      "12200/1431 \tLoss 0.23315846920013428\n",
      "12250/1431 \tLoss 0.24838829040527344\n",
      "12300/1431 \tLoss 0.19641262292861938\n",
      "12350/1431 \tLoss 0.24352213740348816\n",
      "12400/1431 \tLoss 0.17840184271335602\n",
      "12450/1431 \tLoss 0.17569518089294434\n",
      "12500/1431 \tLoss 0.23460114002227783\n",
      "12550/1431 \tLoss 0.22583836317062378\n",
      "12600/1431 \tLoss 0.17778833210468292\n",
      "12650/1431 \tLoss 0.15232600271701813\n",
      "12700/1431 \tLoss 0.2295341193675995\n",
      "12750/1431 \tLoss 0.19422122836112976\n",
      "12800/1431 \tLoss 0.2867956757545471\n",
      "12850/1431 \tLoss 0.1624726802110672\n",
      "Start Training\n",
      "12900/1431 \tLoss 0.11429853737354279\n",
      "12950/1431 \tLoss 0.1809440404176712\n",
      "13000/1431 \tLoss 0.19182991981506348\n",
      "13050/1431 \tLoss 0.2500799298286438\n",
      "13100/1431 \tLoss 0.22226835787296295\n",
      "13150/1431 \tLoss 0.18967850506305695\n",
      "13200/1431 \tLoss 0.22369319200515747\n",
      "13250/1431 \tLoss 0.21208736300468445\n",
      "13300/1431 \tLoss 0.1637084037065506\n",
      "13350/1431 \tLoss 0.2115480601787567\n",
      "13400/1431 \tLoss 0.26206350326538086\n",
      "13450/1431 \tLoss 0.1905442625284195\n",
      "13500/1431 \tLoss 0.23337648808956146\n",
      "13550/1431 \tLoss 0.23310229182243347\n",
      "13600/1431 \tLoss 0.33064666390419006\n",
      "13650/1431 \tLoss 0.3121483325958252\n",
      "13700/1431 \tLoss 0.15224304795265198\n",
      "13750/1431 \tLoss 0.3023216426372528\n",
      "13800/1431 \tLoss 0.2411206066608429\n",
      "13850/1431 \tLoss 0.1532374918460846\n",
      "13900/1431 \tLoss 0.12139996141195297\n",
      "13950/1431 \tLoss 0.20074540376663208\n",
      "14000/1431 \tLoss 0.18485572934150696\n",
      "14050/1431 \tLoss 0.20166437327861786\n",
      "14100/1431 \tLoss 0.25919702649116516\n",
      "14150/1431 \tLoss 0.20393817126750946\n",
      "14200/1431 \tLoss 0.15260277688503265\n",
      "14250/1431 \tLoss 0.22441036999225616\n",
      "14300/1431 \tLoss 0.17825573682785034\n",
      " 99.93%\r"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    val_loss, y_pred, y_true = validate(val_loader, model, criterion)\n",
    "    validation_logs(i, val_loss, y_pred, y_true)\n",
    "    loss = train(\n",
    "                    train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    i,\n",
    "                    on_iteration=on_iteration_logs,\n",
    "                )\n",
    "    logger.add_scalar(\"Loss/Avg_Train\", loss, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, child in enumerate(model.children()):\n",
    "    if i > 6:\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE//2, shuffle=TRAIN_SHUFFLE, num_workers=TRAIN_NUM_WORKERS, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=INITIAL_LR*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "28650/2862 \tLoss 0.18004454672336578\n",
      "28700/2862 \tLoss 0.20700326561927795\n",
      "28750/2862 \tLoss 0.1323094666004181\n",
      "28800/2862 \tLoss 0.15843237936496735\n",
      "28850/2862 \tLoss 0.319430947303772\n",
      "28900/2862 \tLoss 0.25684744119644165\n",
      "28950/2862 \tLoss 0.1464371681213379\n",
      "29000/2862 \tLoss 0.11895129084587097\n",
      "29050/2862 \tLoss 0.19154097139835358\n",
      "29100/2862 \tLoss 0.14050543308258057\n",
      "29150/2862 \tLoss 0.24416416883468628\n",
      "29200/2862 \tLoss 0.1800231635570526\n",
      "29250/2862 \tLoss 0.23812174797058105\n",
      "29300/2862 \tLoss 0.14606578648090363\n",
      "29350/2862 \tLoss 0.2400764524936676\n",
      "29400/2862 \tLoss 0.1481853574514389\n",
      "29450/2862 \tLoss 0.05916711688041687\n",
      "29500/2862 \tLoss 0.2874000370502472\n",
      "29550/2862 \tLoss 0.20592767000198364\n",
      "29600/2862 \tLoss 0.1193256601691246\n",
      "29650/2862 \tLoss 0.21155336499214172\n",
      "29700/2862 \tLoss 0.14782389998435974\n",
      "29750/2862 \tLoss 0.21358148753643036\n",
      "29800/2862 \tLoss 0.2594754695892334\n",
      "29850/2862 \tLoss 0.09838778525590897\n",
      "29900/2862 \tLoss 0.1941651701927185\n",
      "29950/2862 \tLoss 0.17431071400642395\n",
      "30000/2862 \tLoss 0.11102628707885742\n",
      "30050/2862 \tLoss 0.14116214215755463\n",
      "30100/2862 \tLoss 0.11541520059108734\n",
      "30150/2862 \tLoss 0.12993507087230682\n",
      "30200/2862 \tLoss 0.08974899351596832\n",
      "30250/2862 \tLoss 0.14889347553253174\n",
      "30300/2862 \tLoss 0.15337195992469788\n",
      "30350/2862 \tLoss 0.11749888956546783\n",
      "30400/2862 \tLoss 0.19234849512577057\n",
      "30450/2862 \tLoss 0.11080960929393768\n",
      "30500/2862 \tLoss 0.055377960205078125\n",
      "30550/2862 \tLoss 0.17288529872894287\n",
      "30600/2862 \tLoss 0.19722431898117065\n",
      "30650/2862 \tLoss 0.14828960597515106\n",
      "30700/2862 \tLoss 0.09701193869113922\n",
      "30750/2862 \tLoss 0.1849622279405594\n",
      "30800/2862 \tLoss 0.1012611985206604\n",
      "30850/2862 \tLoss 0.1697603464126587\n",
      "30900/2862 \tLoss 0.20232221484184265\n",
      "30950/2862 \tLoss 0.197320818901062\n",
      "31000/2862 \tLoss 0.07255631685256958\n",
      "31050/2862 \tLoss 0.1231132373213768\n",
      "31100/2862 \tLoss 0.07401185482740402\n",
      "31150/2862 \tLoss 0.14343278110027313\n",
      "31200/2862 \tLoss 0.24249371886253357\n",
      "31250/2862 \tLoss 0.12368740886449814\n",
      "31300/2862 \tLoss 0.20510952174663544\n",
      "31350/2862 \tLoss 0.10314130038022995\n",
      "31400/2862 \tLoss 0.16507568955421448\n",
      "31450/2862 \tLoss 0.12911368906497955\n",
      "Start Training\n",
      "31500/2862 \tLoss 0.1300196349620819\n",
      "31550/2862 \tLoss 0.1320495307445526\n",
      "31600/2862 \tLoss 0.21411675214767456\n",
      "31650/2862 \tLoss 0.25623854994773865\n",
      "31700/2862 \tLoss 0.07498446106910706\n",
      "31750/2862 \tLoss 0.18862132728099823\n",
      "31800/2862 \tLoss 0.09698273986577988\n",
      "31850/2862 \tLoss 0.09635821729898453\n",
      "31900/2862 \tLoss 0.11102421581745148\n",
      "31950/2862 \tLoss 0.05471637845039368\n",
      "32000/2862 \tLoss 0.06739374995231628\n",
      "32050/2862 \tLoss 0.15542849898338318\n",
      "32100/2862 \tLoss 0.07416073977947235\n",
      "32150/2862 \tLoss 0.1483050137758255\n",
      "32200/2862 \tLoss 0.11686169356107712\n",
      "32250/2862 \tLoss 0.15061523020267487\n",
      "32300/2862 \tLoss 0.08163531124591827\n",
      "32350/2862 \tLoss 0.1343340128660202\n",
      "32400/2862 \tLoss 0.12455930560827255\n",
      "32450/2862 \tLoss 0.11136190593242645\n",
      "32500/2862 \tLoss 0.17354342341423035\n",
      "32550/2862 \tLoss 0.21878153085708618\n",
      "32600/2862 \tLoss 0.13236285746097565\n",
      "32650/2862 \tLoss 0.08511005342006683\n",
      "32700/2862 \tLoss 0.10796213895082474\n",
      "32750/2862 \tLoss 0.2240017056465149\n",
      "32800/2862 \tLoss 0.11244489252567291\n",
      "32850/2862 \tLoss 0.06817619502544403\n",
      "32900/2862 \tLoss 0.10275496542453766\n",
      "32950/2862 \tLoss 0.24014391005039215\n",
      "33000/2862 \tLoss 0.2560804486274719\n",
      "33050/2862 \tLoss 0.07708053290843964\n",
      "33100/2862 \tLoss 0.19312766194343567\n",
      "33150/2862 \tLoss 0.29280218482017517\n",
      "33200/2862 \tLoss 0.10801170766353607\n",
      "33250/2862 \tLoss 0.0697920173406601\n",
      "33300/2862 \tLoss 0.23118403553962708\n",
      "33350/2862 \tLoss 0.06508058309555054\n",
      "33400/2862 \tLoss 0.16743724048137665\n",
      "33450/2862 \tLoss 0.11403071135282516\n",
      "33500/2862 \tLoss 0.0875939279794693\n",
      "33550/2862 \tLoss 0.104692243039608\n",
      "33600/2862 \tLoss 0.11490572243928909\n",
      "33650/2862 \tLoss 0.13999317586421967\n",
      "33700/2862 \tLoss 0.21749018132686615\n",
      "33750/2862 \tLoss 0.046082012355327606\n",
      "33800/2862 \tLoss 0.04749250039458275\n",
      "33850/2862 \tLoss 0.03909936174750328\n",
      "33900/2862 \tLoss 0.19215819239616394\n",
      "33950/2862 \tLoss 0.09974208474159241\n",
      "34000/2862 \tLoss 0.05269572511315346\n",
      "34050/2862 \tLoss 0.11059905588626862\n",
      "34100/2862 \tLoss 0.1502448320388794\n",
      "34150/2862 \tLoss 0.09541138261556625\n",
      "34200/2862 \tLoss 0.18423178791999817\n",
      "34250/2862 \tLoss 0.18472152948379517\n",
      "34300/2862 \tLoss 0.1545664370059967\n",
      "Start Training\n",
      "34350/2862 \tLoss 0.0878094956278801\n",
      "34400/2862 \tLoss 0.09489183127880096\n",
      "34450/2862 \tLoss 0.11491785198450089\n",
      "34500/2862 \tLoss 0.11933543533086777\n",
      "34550/2862 \tLoss 0.18517367541790009\n",
      "34600/2862 \tLoss 0.1300363838672638\n",
      "34650/2862 \tLoss 0.1028582975268364\n",
      "34700/2862 \tLoss 0.1667090356349945\n",
      "34750/2862 \tLoss 0.13594043254852295\n",
      "34800/2862 \tLoss 0.12948033213615417\n",
      "34850/2862 \tLoss 0.11409533768892288\n",
      "34900/2862 \tLoss 0.16713222861289978\n",
      "34950/2862 \tLoss 0.04796022176742554\n",
      "35000/2862 \tLoss 0.11459869146347046\n",
      "35050/2862 \tLoss 0.17523673176765442\n",
      "35100/2862 \tLoss 0.13531669974327087\n",
      "35150/2862 \tLoss 0.18550193309783936\n",
      "35200/2862 \tLoss 0.07624676823616028\n",
      "35250/2862 \tLoss 0.10727892816066742\n",
      "35300/2862 \tLoss 0.17595821619033813\n",
      "35350/2862 \tLoss 0.09677420556545258\n",
      "35400/2862 \tLoss 0.22944329679012299\n",
      "35450/2862 \tLoss 0.16933664679527283\n",
      "35500/2862 \tLoss 0.15997567772865295\n",
      "35550/2862 \tLoss 0.09266184270381927\n",
      "35600/2862 \tLoss 0.046918466687202454\n",
      "35650/2862 \tLoss 0.17172358930110931\n",
      "35700/2862 \tLoss 0.07529005408287048\n",
      "35750/2862 \tLoss 0.06304900348186493\n",
      "35800/2862 \tLoss 0.06260339915752411\n",
      "35850/2862 \tLoss 0.09771609306335449\n",
      "35900/2862 \tLoss 0.11510131508111954\n",
      "35950/2862 \tLoss 0.07298551499843597\n",
      "36000/2862 \tLoss 0.1618315577507019\n",
      "36050/2862 \tLoss 0.058291006833314896\n",
      "36100/2862 \tLoss 0.07725255191326141\n",
      "36150/2862 \tLoss 0.13223761320114136\n",
      "36200/2862 \tLoss 0.10627392679452896\n",
      "36250/2862 \tLoss 0.17279914021492004\n",
      "36300/2862 \tLoss 0.03942834585905075\n",
      "36350/2862 \tLoss 0.07455447316169739\n",
      "36400/2862 \tLoss 0.08405892550945282\n",
      "36450/2862 \tLoss 0.17955389618873596\n",
      "36500/2862 \tLoss 0.06583711504936218\n",
      "36550/2862 \tLoss 0.03830929100513458\n",
      "36600/2862 \tLoss 0.09844014048576355\n",
      "36650/2862 \tLoss 0.10690560936927795\n",
      "36700/2862 \tLoss 0.14054913818836212\n",
      "36750/2862 \tLoss 0.14333845674991608\n",
      "36800/2862 \tLoss 0.13758403062820435\n",
      "36850/2862 \tLoss 0.08675799518823624\n",
      "36900/2862 \tLoss 0.0979362428188324\n",
      "36950/2862 \tLoss 0.16317537426948547\n",
      "37000/2862 \tLoss 0.08931904286146164\n",
      "37050/2862 \tLoss 0.05674705654382706\n",
      "37100/2862 \tLoss 0.07457752525806427\n",
      "37150/2862 \tLoss 0.13069209456443787\n",
      "37200/2862 \tLoss 0.0847582221031189\n",
      "Start Training\n",
      "37250/2862 \tLoss 0.08439208567142487\n",
      "37300/2862 \tLoss 0.08443085849285126\n",
      "37350/2862 \tLoss 0.21644547581672668\n",
      "37400/2862 \tLoss 0.18623925745487213\n",
      "37450/2862 \tLoss 0.12049329280853271\n",
      "37500/2862 \tLoss 0.08762955665588379\n",
      "37550/2862 \tLoss 0.13056930899620056\n",
      "37600/2862 \tLoss 0.11699412763118744\n",
      "37650/2862 \tLoss 0.13037042319774628\n",
      "37700/2862 \tLoss 0.023388713598251343\n",
      "37750/2862 \tLoss 0.09066705405712128\n",
      "37800/2862 \tLoss 0.18520936369895935\n",
      "37850/2862 \tLoss 0.10453695058822632\n",
      "37900/2862 \tLoss 0.2287725806236267\n",
      "37950/2862 \tLoss 0.06997568160295486\n",
      "38000/2862 \tLoss 0.09540437161922455\n",
      "38050/2862 \tLoss 0.2472701370716095\n",
      "38100/2862 \tLoss 0.13299579918384552\n",
      "38150/2862 \tLoss 0.13845707476139069\n",
      "38200/2862 \tLoss 0.07062125205993652\n",
      "38250/2862 \tLoss 0.13251760601997375\n",
      "38300/2862 \tLoss 0.2308114469051361\n",
      "38350/2862 \tLoss 0.13212314248085022\n",
      "38400/2862 \tLoss 0.12134501338005066\n",
      "38450/2862 \tLoss 0.16344931721687317\n",
      "38500/2862 \tLoss 0.06549593061208725\n",
      "38550/2862 \tLoss 0.06423389911651611\n",
      "38600/2862 \tLoss 0.0759226456284523\n",
      "38650/2862 \tLoss 0.20244790613651276\n",
      "38700/2862 \tLoss 0.07218484580516815\n",
      "38750/2862 \tLoss 0.13511639833450317\n",
      "38800/2862 \tLoss 0.10004901885986328\n",
      "38850/2862 \tLoss 0.13576841354370117\n",
      "38900/2862 \tLoss 0.07227165997028351\n",
      "38950/2862 \tLoss 0.11494573205709457\n",
      "39000/2862 \tLoss 0.09725598245859146\n",
      "39050/2862 \tLoss 0.18342609703540802\n",
      "39100/2862 \tLoss 0.0888853520154953\n",
      "39150/2862 \tLoss 0.08060322701931\n",
      "39200/2862 \tLoss 0.061459675431251526\n",
      "39250/2862 \tLoss 0.0777224600315094\n",
      "39300/2862 \tLoss 0.11906591057777405\n",
      "39350/2862 \tLoss 0.0767398476600647\n",
      "39400/2862 \tLoss 0.06759883463382721\n",
      "39450/2862 \tLoss 0.17583972215652466\n",
      "39500/2862 \tLoss 0.10159967839717865\n",
      "39550/2862 \tLoss 0.1354367882013321\n",
      "39600/2862 \tLoss 0.1404419094324112\n",
      "39650/2862 \tLoss 0.12171968817710876\n",
      "39700/2862 \tLoss 0.11912977695465088\n",
      "39750/2862 \tLoss 0.09156493097543716\n",
      "39800/2862 \tLoss 0.10075749456882477\n",
      "39850/2862 \tLoss 0.0666913092136383\n",
      "39900/2862 \tLoss 0.10192502290010452\n",
      "39950/2862 \tLoss 0.08415146917104721\n",
      "40000/2862 \tLoss 0.06457585841417313\n",
      "40050/2862 \tLoss 0.10542784631252289\n",
      "Start Training\n",
      "40100/2862 \tLoss 0.2296324074268341\n",
      "40150/2862 \tLoss 0.16997326910495758\n",
      "40200/2862 \tLoss 0.05766493082046509\n",
      "40250/2862 \tLoss 0.08657370507717133\n",
      "40300/2862 \tLoss 0.09241540729999542\n",
      "40350/2862 \tLoss 0.23813758790493011\n",
      "40400/2862 \tLoss 0.20156048238277435\n",
      "40450/2862 \tLoss 0.08123096078634262\n",
      "40500/2862 \tLoss 0.05068279057741165\n",
      "40550/2862 \tLoss 0.10887113213539124\n",
      "40600/2862 \tLoss 0.08928433060646057\n",
      "40650/2862 \tLoss 0.09193462133407593\n",
      "40700/2862 \tLoss 0.15140002965927124\n",
      "40750/2862 \tLoss 0.06748951971530914\n",
      "40800/2862 \tLoss 0.08901028335094452\n",
      "40850/2862 \tLoss 0.09017796814441681\n",
      "40900/2862 \tLoss 0.11260270327329636\n",
      "40950/2862 \tLoss 0.1602204442024231\n",
      "41000/2862 \tLoss 0.06411515176296234\n",
      "41050/2862 \tLoss 0.07201606780290604\n",
      "41100/2862 \tLoss 0.07150644063949585\n",
      "41150/2862 \tLoss 0.19560092687606812\n",
      "41200/2862 \tLoss 0.11301393806934357\n",
      "41250/2862 \tLoss 0.11097931116819382\n",
      "41300/2862 \tLoss 0.05570157989859581\n",
      "41350/2862 \tLoss 0.03901544213294983\n",
      "41400/2862 \tLoss 0.07111451774835587\n",
      "41450/2862 \tLoss 0.07505732774734497\n",
      "41500/2862 \tLoss 0.30723533034324646\n",
      "41550/2862 \tLoss 0.03631991520524025\n",
      "41600/2862 \tLoss 0.1491151750087738\n",
      "41650/2862 \tLoss 0.08695138245820999\n",
      "41700/2862 \tLoss 0.08154989778995514\n",
      "41750/2862 \tLoss 0.12331343442201614\n",
      "41800/2862 \tLoss 0.12227780371904373\n",
      "41850/2862 \tLoss 0.09661766141653061\n",
      "41900/2862 \tLoss 0.1305544674396515\n",
      "41950/2862 \tLoss 0.1098201721906662\n",
      "42000/2862 \tLoss 0.10705381631851196\n",
      "42050/2862 \tLoss 0.044235046952962875\n",
      "42100/2862 \tLoss 0.14136908948421478\n",
      "42150/2862 \tLoss 0.14245428144931793\n",
      "42200/2862 \tLoss 0.0575176402926445\n",
      "42250/2862 \tLoss 0.12111812829971313\n",
      "42300/2862 \tLoss 0.11835530400276184\n",
      "42350/2862 \tLoss 0.09410873800516129\n",
      "42400/2862 \tLoss 0.03372742608189583\n",
      "42450/2862 \tLoss 0.17987558245658875\n",
      "42500/2862 \tLoss 0.08608206361532211\n",
      "42550/2862 \tLoss 0.15120035409927368\n",
      "42600/2862 \tLoss 0.1063632220029831\n",
      "42650/2862 \tLoss 0.1121077910065651\n",
      "42700/2862 \tLoss 0.1352289319038391\n",
      "42750/2862 \tLoss 0.0667986199259758\n",
      "42800/2862 \tLoss 0.07837745547294617\n",
      "42850/2862 \tLoss 0.044287167489528656\n",
      "42900/2862 \tLoss 0.09157194942235947\n",
      "Start Training\n",
      "42950/2862 \tLoss 0.1297469139099121\n",
      "43000/2862 \tLoss 0.19636845588684082\n",
      "43050/2862 \tLoss 0.07229363173246384\n",
      "43100/2862 \tLoss 0.11006775498390198\n",
      "43150/2862 \tLoss 0.1414583921432495\n",
      "43200/2862 \tLoss 0.1310325264930725\n",
      "43250/2862 \tLoss 0.10310669243335724\n",
      "43300/2862 \tLoss 0.09027239680290222\n",
      "43350/2862 \tLoss 0.08177797496318817\n",
      "43400/2862 \tLoss 0.06459659337997437\n",
      "43450/2862 \tLoss 0.07987008988857269\n",
      "43500/2862 \tLoss 0.12669095396995544\n",
      "43550/2862 \tLoss 0.13152872025966644\n",
      "43600/2862 \tLoss 0.027665872126817703\n",
      "43650/2862 \tLoss 0.17887818813323975\n",
      "43700/2862 \tLoss 0.08780664205551147\n",
      "43750/2862 \tLoss 0.2036558985710144\n",
      "43800/2862 \tLoss 0.027727117761969566\n",
      "43850/2862 \tLoss 0.14186963438987732\n",
      "43900/2862 \tLoss 0.05914781242609024\n",
      "43950/2862 \tLoss 0.07391487807035446\n",
      "44000/2862 \tLoss 0.14957398176193237\n",
      "44050/2862 \tLoss 0.16696816682815552\n",
      "44100/2862 \tLoss 0.15734589099884033\n",
      "44150/2862 \tLoss 0.061676912009716034\n",
      "44200/2862 \tLoss 0.04843761771917343\n",
      "44250/2862 \tLoss 0.09161071479320526\n",
      "44300/2862 \tLoss 0.10618158429861069\n",
      "44350/2862 \tLoss 0.06732647120952606\n",
      "44400/2862 \tLoss 0.08694296330213547\n",
      "44450/2862 \tLoss 0.11117756366729736\n",
      "44500/2862 \tLoss 0.14043551683425903\n",
      "44550/2862 \tLoss 0.18038591742515564\n",
      "44600/2862 \tLoss 0.06706703454256058\n",
      "44650/2862 \tLoss 0.14086253941059113\n",
      "44700/2862 \tLoss 0.16489963233470917\n",
      "44750/2862 \tLoss 0.07055851817131042\n",
      "44800/2862 \tLoss 0.04359982907772064\n",
      "44850/2862 \tLoss 0.1272040605545044\n",
      "44900/2862 \tLoss 0.18620377779006958\n",
      "44950/2862 \tLoss 0.09366166591644287\n",
      "45000/2862 \tLoss 0.19730131328105927\n",
      "45050/2862 \tLoss 0.040280185639858246\n",
      "45100/2862 \tLoss 0.18762943148612976\n",
      "45150/2862 \tLoss 0.09401535987854004\n",
      "45200/2862 \tLoss 0.05906658619642258\n",
      "45250/2862 \tLoss 0.06692099571228027\n",
      "45300/2862 \tLoss 0.05353480577468872\n",
      "45350/2862 \tLoss 0.15315550565719604\n",
      "45400/2862 \tLoss 0.06798914074897766\n",
      "45450/2862 \tLoss 0.12901851534843445\n",
      "45500/2862 \tLoss 0.0844884142279625\n",
      "45550/2862 \tLoss 0.20617464184761047\n",
      "45600/2862 \tLoss 0.060763388872146606\n",
      "45650/2862 \tLoss 0.07319528609514236\n",
      "45700/2862 \tLoss 0.10971521586179733\n",
      "45750/2862 \tLoss 0.07902970910072327\n",
      "Start Training\n",
      "45800/2862 \tLoss 0.10179854929447174\n",
      "45850/2862 \tLoss 0.14722421765327454\n",
      "45900/2862 \tLoss 0.1084844097495079\n",
      "45950/2862 \tLoss 0.09840938448905945\n",
      "46000/2862 \tLoss 0.19635704159736633\n",
      "46050/2862 \tLoss 0.02419525571167469\n",
      "46100/2862 \tLoss 0.12789657711982727\n",
      "46150/2862 \tLoss 0.07218483090400696\n",
      "46200/2862 \tLoss 0.03666628897190094\n",
      "46250/2862 \tLoss 0.13602223992347717\n",
      "46300/2862 \tLoss 0.10553030669689178\n",
      "46350/2862 \tLoss 0.10534121096134186\n",
      "46400/2862 \tLoss 0.12584663927555084\n",
      "46450/2862 \tLoss 0.0777478814125061\n",
      "46500/2862 \tLoss 0.08597415685653687\n",
      "46550/2862 \tLoss 0.022211847826838493\n",
      "46600/2862 \tLoss 0.05962147191166878\n",
      "46650/2862 \tLoss 0.06511710584163666\n",
      "46700/2862 \tLoss 0.15930047631263733\n",
      "46750/2862 \tLoss 0.0310501828789711\n",
      "46800/2862 \tLoss 0.020670203492045403\n",
      "46850/2862 \tLoss 0.07585538178682327\n",
      "46900/2862 \tLoss 0.040449414402246475\n",
      "46950/2862 \tLoss 0.10701629519462585\n",
      "47000/2862 \tLoss 0.09602752327919006\n",
      "47050/2862 \tLoss 0.0645604133605957\n",
      "47100/2862 \tLoss 0.10329680144786835\n",
      "47150/2862 \tLoss 0.1747397780418396\n",
      "47200/2862 \tLoss 0.06252730637788773\n",
      "47250/2862 \tLoss 0.09492063522338867\n",
      "47300/2862 \tLoss 0.07278509438037872\n",
      "47350/2862 \tLoss 0.076523557305336\n",
      "47400/2862 \tLoss 0.1548536717891693\n",
      "47450/2862 \tLoss 0.13712243735790253\n",
      "47500/2862 \tLoss 0.06216674670577049\n",
      "47550/2862 \tLoss 0.050477802753448486\n",
      "47600/2862 \tLoss 0.06544897705316544\n",
      "47650/2862 \tLoss 0.05850011855363846\n",
      "47700/2862 \tLoss 0.28073498606681824\n",
      "47750/2862 \tLoss 0.06120550259947777\n",
      "47800/2862 \tLoss 0.04236491769552231\n",
      "47850/2862 \tLoss 0.10359972715377808\n",
      "47900/2862 \tLoss 0.13751959800720215\n",
      "47950/2862 \tLoss 0.10444024205207825\n",
      "48000/2862 \tLoss 0.14567624032497406\n",
      "48050/2862 \tLoss 0.059284139424562454\n",
      "48100/2862 \tLoss 0.15847831964492798\n",
      "48150/2862 \tLoss 0.038413919508457184\n",
      "48200/2862 \tLoss 0.07133731991052628\n",
      "48250/2862 \tLoss 0.04447013512253761\n",
      "48300/2862 \tLoss 0.11237046867609024\n",
      "48350/2862 \tLoss 0.10915056616067886\n",
      "48400/2862 \tLoss 0.17948459088802338\n",
      "48450/2862 \tLoss 0.11963462829589844\n",
      "48500/2862 \tLoss 0.06023293361067772\n",
      "48550/2862 \tLoss 0.09889411181211472\n",
      "48600/2862 \tLoss 0.032835960388183594\n",
      "48650/2862 \tLoss 0.1605602353811264\n",
      "Start Training\n",
      "48700/2862 \tLoss 0.07113339006900787\n",
      "48750/2862 \tLoss 0.05609088018536568\n",
      "48800/2862 \tLoss 0.11653030663728714\n",
      "48850/2862 \tLoss 0.11955195665359497\n",
      "48900/2862 \tLoss 0.06552083790302277\n",
      "48950/2862 \tLoss 0.057659730315208435\n",
      "49000/2862 \tLoss 0.054847653955221176\n",
      "49050/2862 \tLoss 0.11241357028484344\n",
      "49100/2862 \tLoss 0.10538339614868164\n",
      "49150/2862 \tLoss 0.07979677617549896\n",
      "49200/2862 \tLoss 0.13058480620384216\n",
      "49250/2862 \tLoss 0.08656489849090576\n",
      "49300/2862 \tLoss 0.13973534107208252\n",
      "49350/2862 \tLoss 0.13646012544631958\n",
      "49400/2862 \tLoss 0.03896699845790863\n",
      "49450/2862 \tLoss 0.07671600580215454\n",
      "49500/2862 \tLoss 0.08646614104509354\n",
      "49550/2862 \tLoss 0.19369885325431824\n",
      "49600/2862 \tLoss 0.17579713463783264\n",
      "49650/2862 \tLoss 0.0552440844476223\n",
      "49700/2862 \tLoss 0.07565730810165405\n",
      "49750/2862 \tLoss 0.1091235876083374\n",
      "49800/2862 \tLoss 0.07827169448137283\n",
      "49850/2862 \tLoss 0.09749861806631088\n",
      "49900/2862 \tLoss 0.08689761161804199\n",
      "49950/2862 \tLoss 0.1279856264591217\n",
      "50000/2862 \tLoss 0.06804826855659485\n",
      "50050/2862 \tLoss 0.1797000914812088\n",
      "50100/2862 \tLoss 0.05455244332551956\n",
      "50150/2862 \tLoss 0.04652849957346916\n",
      "50200/2862 \tLoss 0.1417074203491211\n",
      "50250/2862 \tLoss 0.07075926661491394\n",
      "50300/2862 \tLoss 0.1483953297138214\n",
      "50350/2862 \tLoss 0.036539144814014435\n",
      "50400/2862 \tLoss 0.07629351317882538\n",
      "50450/2862 \tLoss 0.1114799752831459\n",
      "50500/2862 \tLoss 0.08581386506557465\n",
      "50550/2862 \tLoss 0.13453897833824158\n",
      "50600/2862 \tLoss 0.10979297757148743\n",
      "50650/2862 \tLoss 0.10382775962352753\n",
      "50700/2862 \tLoss 0.09079304337501526\n",
      "50750/2862 \tLoss 0.15353697538375854\n",
      "50800/2862 \tLoss 0.12175731360912323\n",
      "50850/2862 \tLoss 0.07103273272514343\n",
      "50900/2862 \tLoss 0.13111984729766846\n",
      "50950/2862 \tLoss 0.1509830206632614\n",
      "51000/2862 \tLoss 0.16367600858211517\n",
      "51050/2862 \tLoss 0.0869298055768013\n",
      "51100/2862 \tLoss 0.16770140826702118\n",
      "51150/2862 \tLoss 0.13083325326442719\n",
      "51200/2862 \tLoss 0.0929441973567009\n",
      "51250/2862 \tLoss 0.10154567658901215\n",
      "51300/2862 \tLoss 0.035023853182792664\n",
      "51350/2862 \tLoss 0.05697092041373253\n",
      "51400/2862 \tLoss 0.07093106210231781\n",
      "51450/2862 \tLoss 0.06885362416505814\n",
      "51500/2862 \tLoss 0.1909208446741104\n",
      "Start Training\n",
      "51550/2862 \tLoss 0.09779134392738342\n",
      "51600/2862 \tLoss 0.14213982224464417\n",
      "51650/2862 \tLoss 0.047508109360933304\n",
      "51700/2862 \tLoss 0.06725172698497772\n",
      "51750/2862 \tLoss 0.12511536478996277\n",
      "51800/2862 \tLoss 0.10878279060125351\n",
      "51850/2862 \tLoss 0.13043507933616638\n",
      "51900/2862 \tLoss 0.03633347153663635\n",
      "51950/2862 \tLoss 0.04158409684896469\n",
      "52000/2862 \tLoss 0.09736505895853043\n",
      "52050/2862 \tLoss 0.10538084805011749\n",
      "52100/2862 \tLoss 0.1245214194059372\n",
      "52150/2862 \tLoss 0.13028864562511444\n",
      "52200/2862 \tLoss 0.055197421461343765\n",
      "52250/2862 \tLoss 0.1610041707754135\n",
      "52300/2862 \tLoss 0.0958174392580986\n",
      "52350/2862 \tLoss 0.10100805759429932\n",
      "52400/2862 \tLoss 0.11943123489618301\n",
      "52450/2862 \tLoss 0.036398403346538544\n",
      "52500/2862 \tLoss 0.07658524066209793\n",
      "52550/2862 \tLoss 0.1271514594554901\n",
      "52600/2862 \tLoss 0.09229940176010132\n",
      "52650/2862 \tLoss 0.10492300242185593\n",
      "52700/2862 \tLoss 0.069602832198143\n",
      "52750/2862 \tLoss 0.09559344500303268\n",
      "52800/2862 \tLoss 0.06815517693758011\n",
      "52850/2862 \tLoss 0.03687720000743866\n",
      "52900/2862 \tLoss 0.21418681740760803\n",
      "52950/2862 \tLoss 0.21334972977638245\n",
      "53000/2862 \tLoss 0.18868231773376465\n",
      "53050/2862 \tLoss 0.10150308161973953\n",
      "53100/2862 \tLoss 0.04878508299589157\n",
      "53150/2862 \tLoss 0.04382835328578949\n",
      "53200/2862 \tLoss 0.038313597440719604\n",
      "53250/2862 \tLoss 0.14247643947601318\n",
      "53300/2862 \tLoss 0.18435490131378174\n",
      "53350/2862 \tLoss 0.08127596229314804\n",
      "53400/2862 \tLoss 0.09165029227733612\n",
      "53450/2862 \tLoss 0.13781540095806122\n",
      "53500/2862 \tLoss 0.026936979964375496\n",
      "53550/2862 \tLoss 0.024725794792175293\n",
      "53600/2862 \tLoss 0.0773567482829094\n",
      "53650/2862 \tLoss 0.07048370689153671\n",
      "53700/2862 \tLoss 0.11446206271648407\n",
      "53750/2862 \tLoss 0.029875244945287704\n",
      "53800/2862 \tLoss 0.05886756256222725\n",
      "53850/2862 \tLoss 0.13990944623947144\n",
      "53900/2862 \tLoss 0.11609113216400146\n",
      "53950/2862 \tLoss 0.04374368488788605\n",
      "54000/2862 \tLoss 0.038168907165527344\n",
      "54050/2862 \tLoss 0.038191087543964386\n",
      "54100/2862 \tLoss 0.0499248243868351\n",
      "54150/2862 \tLoss 0.08417337387800217\n",
      "54200/2862 \tLoss 0.019357651472091675\n",
      "54250/2862 \tLoss 0.10029558837413788\n",
      "54300/2862 \tLoss 0.09091715514659882\n",
      "54350/2862 \tLoss 0.036033745855093\n",
      "Start Training\n",
      "54400/2862 \tLoss 0.11504608392715454\n",
      "54450/2862 \tLoss 0.08799879997968674\n",
      "54500/2862 \tLoss 0.10771773755550385\n",
      "54550/2862 \tLoss 0.07177627831697464\n",
      "54600/2862 \tLoss 0.1600388139486313\n",
      "54650/2862 \tLoss 0.04053310304880142\n",
      "54700/2862 \tLoss 0.08255933225154877\n",
      "54750/2862 \tLoss 0.037299007177352905\n",
      "54800/2862 \tLoss 0.12186721712350845\n",
      "54850/2862 \tLoss 0.11478860676288605\n",
      "54900/2862 \tLoss 0.06755673885345459\n",
      "54950/2862 \tLoss 0.1174277737736702\n",
      "55000/2862 \tLoss 0.1320161074399948\n",
      "55050/2862 \tLoss 0.0753311812877655\n",
      "55100/2862 \tLoss 0.11122795939445496\n",
      "55150/2862 \tLoss 0.07863280177116394\n",
      "55200/2862 \tLoss 0.1646469533443451\n",
      "55250/2862 \tLoss 0.05967273190617561\n",
      "55300/2862 \tLoss 0.09528933465480804\n",
      "55350/2862 \tLoss 0.11857572942972183\n",
      "55400/2862 \tLoss 0.014580221846699715\n",
      "55450/2862 \tLoss 0.08919531106948853\n",
      "55500/2862 \tLoss 0.1138274148106575\n",
      "55550/2862 \tLoss 0.12088654935359955\n",
      "55600/2862 \tLoss 0.11861961334943771\n",
      "55650/2862 \tLoss 0.058036260306835175\n",
      "55700/2862 \tLoss 0.1063893735408783\n",
      "55750/2862 \tLoss 0.05788755416870117\n",
      "55800/2862 \tLoss 0.07524330914020538\n",
      "55850/2862 \tLoss 0.11869688332080841\n",
      "55900/2862 \tLoss 0.07413385063409805\n",
      "55950/2862 \tLoss 0.1272798627614975\n",
      "56000/2862 \tLoss 0.01302160695195198\n",
      "56050/2862 \tLoss 0.06083602458238602\n",
      "56100/2862 \tLoss 0.04902594909071922\n",
      "56150/2862 \tLoss 0.07225090265274048\n",
      "56200/2862 \tLoss 0.10922450572252274\n",
      "56250/2862 \tLoss 0.05298541858792305\n",
      "56300/2862 \tLoss 0.058663561940193176\n",
      "56350/2862 \tLoss 0.049482010304927826\n",
      "56400/2862 \tLoss 0.11410506814718246\n",
      "56450/2862 \tLoss 0.079591765999794\n",
      "56500/2862 \tLoss 0.11794173717498779\n",
      "56550/2862 \tLoss 0.0999949499964714\n",
      "56600/2862 \tLoss 0.16536283493041992\n",
      "56650/2862 \tLoss 0.07281990349292755\n",
      "56700/2862 \tLoss 0.13915470242500305\n",
      "56750/2862 \tLoss 0.05237475782632828\n",
      "56800/2862 \tLoss 0.09565271437168121\n",
      "56850/2862 \tLoss 0.10232992470264435\n",
      "56900/2862 \tLoss 0.06067051365971565\n",
      "56950/2862 \tLoss 0.05339738354086876\n",
      "57000/2862 \tLoss 0.10333803296089172\n",
      "57050/2862 \tLoss 0.16247916221618652\n",
      "57100/2862 \tLoss 0.02244791015982628\n",
      "57150/2862 \tLoss 0.03930051252245903\n",
      "57200/2862 \tLoss 0.08535583317279816\n",
      " 99.97%\r"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    val_loss, y_pred, y_true = validate(val_loader, model, criterion)\n",
    "    validation_logs(i, val_loss, y_pred, y_true)\n",
    "    loss = train(\n",
    "                    train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    i,\n",
    "                    on_iteration=on_iteration_logs,\n",
    "                )\n",
    "    logger.add_scalar(\"Loss/Avg_Train\", loss, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.resnext101_32x8d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, child in enumerate(model.children()):\n",
    "    if i > 3:\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad=True\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=INITIAL_LR*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,30):\n",
    "    val_loss, y_pred, y_true = validate(val_loader, model, criterion)\n",
    "    validation_logs(i, val_loss, y_pred, y_true)\n",
    "    loss = train(\n",
    "                    train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    i,\n",
    "                    on_iteration=on_iteration_logs,\n",
    "                )\n",
    "    logger.add_scalar(\"Loss/Avg_Train\", loss, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
